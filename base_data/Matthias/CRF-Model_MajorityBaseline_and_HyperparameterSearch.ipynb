{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd806af",
   "metadata": {},
   "source": [
    "# NER with Conditional Random Fields\n",
    "This notebook is heavily inspired by the work of ***Vasco Ribeiro*** as well as\n",
    "- the `sklearn-crfsuite` [tutorial](https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html) on Named Entity Recognition (NER),\n",
    "- a related NER tutorial using [eli5](https://eli5.readthedocs.io/en/latest/tutorials/sklearn_crfsuite.html),\n",
    "- and [`scikit-learn`](https://scikit-learn.org/0.23/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV).\n",
    "\n",
    "Import libraries and inspect the data in the *annotations.jsonl* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c4eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instances:\n",
      "140\n",
      "\n",
      "all keys:\n",
      "['text', 'meta', '_input_hash', '_task_hash', 'spans', 'tokens', '_view_id', 'answer', '_timestamp']\n",
      "\n",
      "important keys:\n",
      "['text', 'spans', 'tokens']\n",
      "\n",
      "example text:\n",
      "DORNBIRN In der Schulgasse in Dornbirn hat eine 71,93 Quadratmeter große Wohnung für einen Quadratmeterpreis von 5533,71 Euro den Besitzer gewechselt. Dieser beinhaltet auch einen Pkw-Abstellplatz. Käufer der Wohnung mit 9,86 Quadratmetern Terrasse ist die ValLiLean Beteiligungs- und Immobilienverwaltungs GmbH. Beim Verkäufer handelt es sich um die Karrenblick Projekt GmbH.  Der Kaufpreis liegt bei 398.040 Euro. Unterzeichnet wurde der Kaufvertrag am 18. September. Die Verbücherung datiert mit Oktober 2020.\n",
      "\n",
      "5 example spans:\n",
      "{'text': 'DORNBIRN', 'start': 0, 'end': 8, 'pattern': 2069086582, 'token_start': 0, 'token_end': 0, 'label': 'ORT'}\n",
      "{'start': 16, 'end': 26, 'token_start': 3, 'token_end': 3, 'label': 'STRASSE'}\n",
      "{'text': 'Dornbirn', 'start': 30, 'end': 38, 'pattern': 2069086582, 'token_start': 5, 'token_end': 5, 'label': 'ORT'}\n",
      "{'start': 48, 'end': 53, 'token_start': 8, 'token_end': 8, 'label': 'FLAECHE'}\n",
      "{'start': 73, 'end': 80, 'token_start': 11, 'token_end': 11, 'label': 'IMMO_TYP'}\n",
      "\n",
      "5 example tokens:\n",
      "{'text': 'DORNBIRN', 'start': 0, 'end': 8, 'id': 0, 'ws': True}\n",
      "{'text': 'In', 'start': 9, 'end': 11, 'id': 1, 'ws': True}\n",
      "{'text': 'der', 'start': 12, 'end': 15, 'id': 2, 'ws': True}\n",
      "{'text': 'Schulgasse', 'start': 16, 'end': 26, 'id': 3, 'ws': True}\n",
      "{'text': 'in', 'start': 27, 'end': 29, 'id': 4, 'ws': True}\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "import scipy.stats\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open(\"../annotations.jsonl\") as jsonl_file:\n",
    "    lines = jsonl_file.readlines()\n",
    "annot = [json.loads(line) for line in lines]\n",
    "print(\"instances:\\n{}\".format(len(annot)))\n",
    "keys = [key for key in annot[0].keys()]\n",
    "print(\"\\nall keys:\\n{}\".format(keys))\n",
    "key_keys = [\"text\", \"spans\", \"tokens\"]\n",
    "print(\"\\nimportant keys:\\n{}\".format(key_keys))\n",
    "print(\"\\nexample text:\\n{}\".format(annot[0][\"text\"]))\n",
    "n_examples = 5\n",
    "print(\"\\n{} example spans:\".format(n_examples))\n",
    "for span in annot[0][\"spans\"][:n_examples]:\n",
    "    print(\"{}\".format(span))\n",
    "print(\"\\n{} example tokens:\".format(n_examples))\n",
    "for token in annot[0][\"tokens\"][:n_examples]:\n",
    "    print(\"{}\".format(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf5139",
   "metadata": {},
   "source": [
    "There are 140 instances of real estate offerings. Each instance is represented in the form of a dictionary with keys. Important keys are\n",
    "- `text`, with the original text as value,\n",
    "- `spans`, with a list of dictionaries as values, where each dictionary represents an annotation obtained via [prodigy](https://prodi.gy/), and\n",
    "- `tokens`, again with a list of dictionaries as values, but now, every dictionary represents a token.\n",
    "\n",
    "The above output shows an example.\n",
    "\n",
    "## Data Preprocessing\n",
    "Given a list of span dictionaries (key `spans`) and an index referring to the text position within the real estate offering text, return the prodigy label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67f6ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for position -1:\tO\n",
      "label for position 0:\tORT\n",
      "label for position 1:\tORT\n",
      "label for position 2:\tORT\n",
      "label for position 3:\tORT\n",
      "label for position 4:\tO\n",
      "label for position 5:\tORT2\n",
      "label for position 6:\tORT2\n",
      "label for position 7:\tORT2\n",
      "label for position 8:\tORT2\n",
      "label for position 9:\tO\n",
      "label for position 10:\tORT3\n",
      "label for position 11:\tORT3\n",
      "label for position 12:\tORT3\n",
      "label for position 13:\tORT3\n",
      "label for position 14:\tO\n"
     ]
    }
   ],
   "source": [
    "def getLabel(tokenDictList, idx):\n",
    "    result = \"O\"\n",
    "    for dict_i in tokenDictList:\n",
    "        idx_0, idx_1 = dict_i['start'], dict_i['end']\n",
    "        if (idx_0<=idx) and (idx<=idx_1):\n",
    "            result = dict_i[\"label\"]\n",
    "    return result \n",
    "\n",
    "myDictList = [\n",
    "    {'start':0, 'end':3, 'label': 'ORT'},\n",
    "    {'start':5, 'end':8, 'label': 'ORT2'},\n",
    "    {'start':10, 'end':13, 'label': 'ORT3'}\n",
    "]\n",
    "\n",
    "for i in range(16):\n",
    "    pos = i-1\n",
    "    print(\"label for position {}:\\t{}\".format(pos, getLabel(myDictList, i-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd8571",
   "metadata": {},
   "source": [
    "Enhance the `tokens` dictionaries by labels. The labels are stored in the `spans` dictionaries. Unless the trivial class, *O* is assigned to a token, a prepending *I-* (*B-*) denotes the token to be the initial (a subsequent) one in the labelled expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471e9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token dictionaries for the last 3 words of instance 0\n",
      "{'text': 'DORNBIRN', 'start': 0, 'end': 8, 'id': 0, 'ws': True, 'label': 'B-ORT'}\n",
      "{'text': 'In', 'start': 9, 'end': 11, 'id': 1, 'ws': True, 'label': 'O'}\n",
      "{'text': 'der', 'start': 12, 'end': 15, 'id': 2, 'ws': True, 'label': 'O'}\n",
      "Token dictionaries for the last 3 words of instance 1\n",
      "{'text': 'FELDKIRCH', 'start': 0, 'end': 9, 'id': 0, 'ws': True, 'label': 'B-ORT'}\n",
      "{'text': 'Im', 'start': 10, 'end': 12, 'id': 1, 'ws': True, 'label': 'O'}\n",
      "{'text': 'Altenreuteweg', 'start': 13, 'end': 26, 'id': 2, 'ws': True, 'label': 'B-STRASSE'}\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(annot)): # loop over instances\n",
    "    a = annot[j]            # instance j\n",
    "    spans = a['spans']      # list of annotation dicts\n",
    "    toks = a['tokens']      # list of token dicts\n",
    "    for i in range(len(toks)):                                 # loop over token dicts\n",
    "        toks[i]['label'] = getLabel(spans, toks[i]['start'])   # assign label from span (if exists, otherwise \"O\")\n",
    "        if toks[i]['label'] != \"O\":                            # if the token represents an entity ...\n",
    "            if i==0:\n",
    "                toks[i]['label'] = \"B-\"+toks[i]['label']       # ... and is the first in the text => \"B-\" + label\n",
    "            else:                                              # not first token in text:\n",
    "                if (toks[i]['label']==toks[i-1]['label'][2:]):\n",
    "                    toks[i]['label'] = \"I-\"+toks[i]['label']   # > but same label as previous token => \"I-\" + label\n",
    "                else:\n",
    "                    toks[i]['label'] = \"B-\"+toks[i]['label']   # > but first token of an entity => \"B-\" + label\n",
    "    annot[j]['tokens'] = toks\n",
    "\n",
    "words_n = 3\n",
    "for i in range(2):\n",
    "    print(\"Token dictionaries for the last {} words of instance {}\".format(words_n, i))\n",
    "    ann = annot[i]\n",
    "    for tok in ann[\"tokens\"][:words_n]:\n",
    "        print(tok)\n",
    "# O => trivial class (no entity)\n",
    "# B => Entity or leading token of an entity\n",
    "# I => subsequent token of an entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896acdea",
   "metadata": {},
   "source": [
    "Use the enhanced list of *token* dictionaries to create a list where each instance is represented by a list of *(token, label)* pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173023ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('DORNBIRN', 'B-ORT'),\n",
       "  ('In', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Schulgasse', 'B-STRASSE'),\n",
       "  ('in', 'O'),\n",
       "  ('Dornbirn', 'B-ORT'),\n",
       "  ('hat', 'O'),\n",
       "  ('eine', 'O'),\n",
       "  ('71,93', 'B-FLAECHE'),\n",
       "  ('Quadratmeter', 'O'),\n",
       "  ('große', 'O'),\n",
       "  ('Wohnung', 'B-IMMO_TYP'),\n",
       "  ('für', 'O'),\n",
       "  ('einen', 'O'),\n",
       "  ('Quadratmeterpreis', 'O'),\n",
       "  ('von', 'O'),\n",
       "  ('5533,71', 'B-GESAMTPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('den', 'O'),\n",
       "  ('Besitzer', 'O'),\n",
       "  ('gewechselt', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Dieser', 'O'),\n",
       "  ('beinhaltet', 'O'),\n",
       "  ('auch', 'O'),\n",
       "  ('einen', 'O'),\n",
       "  ('Pkw-Abstellplatz', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Käufer', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Wohnung', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('9,86', 'B-TERRASSENGROESSE'),\n",
       "  ('Quadratmetern', 'O'),\n",
       "  ('Terrasse', 'O'),\n",
       "  ('ist', 'O'),\n",
       "  ('die', 'O'),\n",
       "  ('ValLiLean', 'B-KAEUFER'),\n",
       "  ('Beteiligungs-', 'I-KAEUFER'),\n",
       "  ('und', 'I-KAEUFER'),\n",
       "  ('Immobilienverwaltungs', 'I-KAEUFER'),\n",
       "  ('GmbH.', 'I-KAEUFER'),\n",
       "  ('Beim', 'O'),\n",
       "  ('Verkäufer', 'O'),\n",
       "  ('handelt', 'O'),\n",
       "  ('es', 'O'),\n",
       "  ('sich', 'O'),\n",
       "  ('um', 'O'),\n",
       "  ('die', 'O'),\n",
       "  ('Karrenblick', 'B-VERKAEUFER'),\n",
       "  ('Projekt', 'I-VERKAEUFER'),\n",
       "  ('GmbH.', 'I-VERKAEUFER'),\n",
       "  (' ', 'O'),\n",
       "  ('Der', 'O'),\n",
       "  ('Kaufpreis', 'O'),\n",
       "  ('liegt', 'O'),\n",
       "  ('bei', 'O'),\n",
       "  ('398.040', 'B-GESAMTPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Unterzeichnet', 'O'),\n",
       "  ('wurde', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Kaufvertrag', 'O'),\n",
       "  ('am', 'O'),\n",
       "  ('18.', 'B-DATUM_VERTRAG'),\n",
       "  ('September', 'I-DATUM_VERTRAG'),\n",
       "  ('.', 'I-DATUM_VERTRAG'),\n",
       "  ('Die', 'O'),\n",
       "  ('Verbücherung', 'O'),\n",
       "  ('datiert', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('Oktober', 'B-DATUM_VERBUECHERUNG'),\n",
       "  ('2020.', 'I-DATUM_VERBUECHERUNG')],\n",
       " [('FELDKIRCH', 'B-ORT'),\n",
       "  ('Im', 'O'),\n",
       "  ('Altenreuteweg', 'B-STRASSE'),\n",
       "  ('in', 'O'),\n",
       "  ('Feldkirch', 'B-ORT'),\n",
       "  ('hat', 'O'),\n",
       "  ('eine', 'O'),\n",
       "  ('100,67', 'B-FLAECHE'),\n",
       "  ('Quadratmeter', 'O'),\n",
       "  ('große', 'O'),\n",
       "  ('Wohnung', 'B-IMMO_TYP'),\n",
       "  ('für', 'O'),\n",
       "  ('einen', 'O'),\n",
       "  ('Quadratmeterpreis', 'O'),\n",
       "  ('von', 'O'),\n",
       "  ('6168,67', 'B-QMPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('den', 'O'),\n",
       "  ('Besitzer', 'O'),\n",
       "  ('gewechselt', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Käufer', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Wohnung', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('einer', 'O'),\n",
       "  ('137,49', 'B-TERRASSENGROESSE'),\n",
       "  ('Quadratmeter', 'O'),\n",
       "  ('großen', 'O'),\n",
       "  ('Terrasse', 'O'),\n",
       "  ('ist', 'O'),\n",
       "  ('eine', 'O'),\n",
       "  ('Privatperson', 'B-KAEUFER'),\n",
       "  ('.', 'I-KAEUFER'),\n",
       "  ('Beim', 'O'),\n",
       "  ('Verkäufer', 'O'),\n",
       "  ('handelt', 'O'),\n",
       "  ('es', 'O'),\n",
       "  ('sich', 'O'),\n",
       "  ('um', 'O'),\n",
       "  ('die', 'O'),\n",
       "  ('Rüscher', 'B-VERKAEUFER'),\n",
       "  ('und', 'I-VERKAEUFER'),\n",
       "  ('Söhne', 'I-VERKAEUFER'),\n",
       "  ('Bau', 'I-VERKAEUFER'),\n",
       "  ('GmbH', 'I-VERKAEUFER'),\n",
       "  ('&', 'I-VERKAEUFER'),\n",
       "  ('Co', 'I-VERKAEUFER'),\n",
       "  ('KG', 'I-VERKAEUFER'),\n",
       "  ('.', 'I-VERKAEUFER'),\n",
       "  (' ', 'O'),\n",
       "  ('Der', 'O'),\n",
       "  ('Kaufpreis', 'O'),\n",
       "  ('liegt', 'O'),\n",
       "  ('bei', 'O'),\n",
       "  ('621.000', 'B-GESAMTPREIS'),\n",
       "  ('Euro', 'O'),\n",
       "  ('.', 'O'),\n",
       "  ('Unterzeichnet', 'O'),\n",
       "  ('wurde', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Kaufvertrag', 'O'),\n",
       "  ('am', 'O'),\n",
       "  ('11.', 'B-DATUM_VERTRAG'),\n",
       "  ('August', 'I-DATUM_VERTRAG'),\n",
       "  ('.', 'I-DATUM_VERTRAG'),\n",
       "  ('Die', 'O'),\n",
       "  ('Verbücherung', 'O'),\n",
       "  ('datiert', 'O'),\n",
       "  ('mit', 'O'),\n",
       "  ('September', 'B-DATUM_VERBUECHERUNG'),\n",
       "  ('2020.', 'I-DATUM_VERBUECHERUNG')]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents=[] \n",
    "for annot_i in annot:                  # loop over instances\n",
    "    toks = annot_i['tokens']           # get tokens list for instance i\n",
    "    train_sentence = []\n",
    "    for tok in toks:                   # loop over token dicts\n",
    "        if 'label' in tok:             # only if the current token has been labelled, ...\n",
    "            token_element = (tok['text'], tok['label']) # ... create a \"text\", \"label\" pair for this token ...\n",
    "            train_sentence.append(token_element)        # ... and append it to the list\n",
    "    sents.append(train_sentence) # append the list for that instances to the list for all instances / sentences\n",
    "\n",
    "# list of lists of pairs (sets): outer list contains instances and inner list contains (token, label) pairs\n",
    "sents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4783dc7",
   "metadata": {},
   "source": [
    "Now, turn each token into features ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf600b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word2features(sent, i): # receive instance and the index i for the i-th token of the instance\n",
    "    word = sent[i][0]       # i-th token (\"sent\" is a list of (token, label) pairs; the label is not used, here)\n",
    "    # dictionary of features for the i-th token\n",
    "    features = {\n",
    "        'bias': 1.0,                      # a different bias could be computed here (static 1 is useless)\n",
    "        'word.lower()': word.lower(),     # token in lowercase\n",
    "        'word[-3:]': word[-3:],           # last 3 letters\n",
    "        'word[-2:]': word[-2:],           # last 2 letters\n",
    "        'word.isupper()': word.isupper(), # True if uppercase else False\n",
    "        'word.istitle()': word.istitle(), # True if title else False, see...\n",
    "        # ... https://www.w3schools.com/python/trypython.asp?filename=demo_ref_string_istitle2\n",
    "        'word.isdigit()': word.isdigit()  # True if digit else False\n",
    "    }\n",
    "    if i > 0:\n",
    "        # add features for the previous token\n",
    "        word_minus_1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word_minus_1.lower(),\n",
    "            '-1:word.istitle()': word_minus_1.istitle(),\n",
    "            '-1:word.isupper()': word_minus_1.isupper(),\n",
    "        })\n",
    "    else: # the beginning of the sequence\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        # add features for the next token\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else: # the end of the sequence\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):                                      # pass list of (token, label) pairs\n",
    "    return [word2features(sent, i) for i in range(len(sent))] # pass list of (token, label) pairs and the index ...\n",
    "    # ... for a position in that list => obtain dict with token features => return list of such feature dicts\n",
    "\n",
    "# obtain list of dicts with features for the corresponding tokens\n",
    "len(sent2features(sents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ece56d",
   "metadata": {},
   "source": [
    "... and - independently - each label into features. Use these functions to build the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded83837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "{'bias': 1.0, 'word.lower()': 'dornbirn', 'word[-3:]': 'IRN', 'word[-2:]': 'RN', 'word.isupper()': True, 'word.istitle()': False, 'word.isdigit()': False, 'BOS': True, '+1:word.lower()': 'in', '+1:word.istitle()': True, '+1:word.isupper()': False}\n",
      "{'bias': 1.0, 'word.lower()': 'in', 'word[-3:]': 'In', 'word[-2:]': 'In', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, '-1:word.lower()': 'dornbirn', '-1:word.istitle()': False, '-1:word.isupper()': True, '+1:word.lower()': 'der', '+1:word.istitle()': False, '+1:word.isupper()': False}\n",
      "{'bias': 1.0, 'word.lower()': 'der', 'word[-3:]': 'der', 'word[-2:]': 'er', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'in', '-1:word.istitle()': True, '-1:word.isupper()': False, '+1:word.lower()': 'schulgasse', '+1:word.istitle()': True, '+1:word.isupper()': False}\n",
      "{'bias': 1.0, 'word.lower()': 'schulgasse', 'word[-3:]': 'sse', 'word[-2:]': 'se', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, '-1:word.lower()': 'der', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'in', '+1:word.istitle()': False, '+1:word.isupper()': False}\n",
      "\n",
      "140\n",
      "B-ORT\n",
      "O\n",
      "O\n",
      "B-STRASSE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sent2labels(sent):   # pass one instance (=list of (token, label) pairs) => return list of labels\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "X = [sent2features(s) for s in sents] # list of instances, each a list of tokens represented by a feature dict ...\n",
    "                                      # ... => list of dictionaries\n",
    "y = [sent2labels(s) for s in sents]   # list of instances, each a list of tokens represented by a label\n",
    "print(len(X))\n",
    "[print(X[0][i]) for i in range(4)]\n",
    "print(\"\")\n",
    "print(len(y))\n",
    "[print(y[0][i]) for i in range(4)]\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282a45c",
   "metadata": {},
   "source": [
    "Split the data into sets for training (3/4) and testing (1/4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b847d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 104, 36, 36)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratio = 0.75\n",
    "train_test_split = round(0.75*len(X) - 0.5) # -0.5 => floor\n",
    "train_test_split\n",
    "X_train = X[:train_test_split]\n",
    "y_train = y[:train_test_split]\n",
    "X_test = X[train_test_split:]\n",
    "y_test = y[train_test_split:]\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05206689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VR ADDED March 30th\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_ratio = 0.75\n",
    "train_test_split = round(0.75*len(X_all) - 0.5) # -0.5 => floor\n",
    "idx = [i for i in range(0,len(X_all))]\n",
    "idx_shuffle = shuffle(idx,random_state=0)\n",
    "X_shuffle, y_shuffle = [X_all[auxIdx] for auxIdx in idx_shuffle], [y_all[auxIdx] for auxIdx in idx_shuffle]\n",
    "\n",
    "X_train, X_test, y_train, y_test = X_shuffle[:train_test_split], X_shuffle[train_test_split:], y_shuffle[:train_test_split], y_shuffle[train_test_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16388a77",
   "metadata": {},
   "source": [
    "Use a dictionary to track the occurrence counts of the labels and show them in a sorted way. The list of labels for the test set is also required further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddeeb8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts\t label\n",
      "\n",
      "1229\t O\n",
      "15\t B-DATUM_VERBUECHERUNG\n",
      "15\t I-DATUM_VERBUECHERUNG\n",
      "15\t B-DATUM_VERTRAG\n",
      "28\t I-DATUM_VERTRAG\n",
      "17\t B-FLAECHE\n",
      "22\t B-GESAMTPREIS\n",
      "22\t B-IMMO_TYP\n",
      "1\t I-IMMO_TYP\n",
      "15\t B-KAEUFER\n",
      "21\t I-KAEUFER\n",
      "49\t B-ORT\n",
      "14\t I-ORT\n",
      "11\t B-QMPREIS\n",
      "14\t B-STRASSE\n",
      "5\t I-STRASSE\n",
      "5\t B-TERRASSENGROESSE\n",
      "12\t B-VERKAEUFER\n",
      "19\t I-VERKAEUFER\n"
     ]
    }
   ],
   "source": [
    "test_label_dict = {}\n",
    "for y in y_test:\n",
    "    for y_i in y:\n",
    "        if y_i not in test_label_dict:\n",
    "            test_label_dict[y_i] = 1\n",
    "        else:\n",
    "            test_label_dict[y_i] += 1\n",
    "\n",
    "test_labels = [key for key in test_label_dict.keys()]        # dict keys to list\n",
    "test_labels = sorted(test_labels, key=lambda name: name[1:]) # sort list\n",
    "\n",
    "print(\"counts\\t label\\n\")\n",
    "for label in test_labels:\n",
    "    label_count = 0\n",
    "    if label in test_label_dict:\n",
    "        label_count = test_label_dict[label]\n",
    "    print(\"{}\\t {}\".format(label_count, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65c830",
   "metadata": {},
   "source": [
    "We are done with data preprocessing.\n",
    "\n",
    "## Majority baseline\n",
    "The baseline model is a basic reference model to which more sophisticated models are compared. When a model performs worse than the baseline model, something clearly went wrong. As usual with NER, most words / tokens are actually not entities, i.e., most of them belong to the default / trivial class. Thus, a basic baseline model can be obtained by having it always predict the trivial class. `MajorityBaseline` below does exactly that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08457eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MajorityBaseline:\n",
    "    \"\"\"\n",
    "    Naive model that simply predicts the most frequent label in the training data.\n",
    "    \n",
    "    attributes:\n",
    "        self.dict: internal dictionary with training labels as keys and total label occurrences as values\n",
    "        self.dictMax: occurrence count of the most prominent label in the training data; initialized to 0\n",
    "        self.pred: value predicted by this baseline model (initialized to None and updated during training)\n",
    "    \n",
    "    methods:\n",
    "        train: receives X_train and y_train as arguments but X_train is only passed for conformity (not\n",
    "            actually required); updates self.dict, self.dictMax and self.pred\n",
    "        predict: receives X as an argument and returns copies of self.pred in matching format; requires\n",
    "            that self.pred is not None => train method must be called beforehand\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict = {}\n",
    "        self.dictMax = 0\n",
    "        self.pred = None\n",
    "\n",
    "    def train(self, X_train, y_train): # X_train included for conformity but not required\n",
    "        # build dictionary: dict[label] = number of label's occurrences\n",
    "        for y_i in y_train:\n",
    "            for label in y_i:\n",
    "                if label not in self.dict:\n",
    "                    self.dict[label] = 1\n",
    "                else:\n",
    "                    self.dict[label] += 1\n",
    "        # dictionary is built => find the most frequent label\n",
    "        for label in self.dict:\n",
    "            label_occurrences = self.dict[label]\n",
    "            if label_occurrences > self.dictMax:\n",
    "                self.dictMax = label_occurrences # update the max occurrence count of the most frequent label\n",
    "                self.pred = label                # initialize / update the label that this baseline model predicts\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.pred == None:\n",
    "            print(\"ERROR: MajorityBaseline has not been trained, yet!\")\n",
    "        else:\n",
    "            instances = []\n",
    "            for x_i in X:\n",
    "                labels = []\n",
    "                for x_ij in x_i:\n",
    "                    labels.append(self.pred)\n",
    "                instances.append(labels)\n",
    "        return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701bb37d",
   "metadata": {},
   "source": [
    "Instantiate and train the model. Then make predictions and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49f59061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority baseline: 0.716361132203078\n"
     ]
    }
   ],
   "source": [
    "baselineModel = MajorityBaseline()\n",
    "baselineModel.train(X_train, y_train)\n",
    "y_test_pred = baselineModel.predict(X_test)\n",
    "baseline_score = metrics.flat_f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(\"Majority baseline: {}\".format(baseline_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7cdd6",
   "metadata": {},
   "source": [
    "So the above number is the baseline for the $F_1$ score. Hopefully, a model with Conditional Random Fields (CRF) can outperform the baseline model.\n",
    "\n",
    "## Conditional Random Fields with Hyperparameter Search\n",
    "We use the `sklearn_crfsuite` library and explore the hyperparameter space via `RandomizedSearchCV` from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90e1d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthias/opt/anaconda3/envs/nlu/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:   32.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   58.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 333 ms, total: 10.3 s\n",
      "Wall time: 1min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=True, c1=0.01, c2=0.01,\n",
       "    epsilon=0.001, keep_tempfiles=None, max_iterations=300, num_memories=6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(max_iterations=300)\n",
    "params_space = {\n",
    "    'algorithm': ['lbfgs', 'l2sgd', 'ap', 'pa', 'arow'],\n",
    "    'c1': [1.0, 0.1, 0.01],\n",
    "    'c2': [1.0, 0.1, 0.01],\n",
    "    'all_possible_states': [True, False],\n",
    "    'num_memories': [4,5,6,7,8],\n",
    "    'epsilon': [1e-6, 1e-5, 1e-4, 1e-3,],\n",
    "}\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, average='weighted')\n",
    "# search\n",
    "search = RandomizedSearchCV(crf, params_space, cv=3, verbose=1, n_jobs=-1, n_iter=100, scoring=f1_scorer)\n",
    "#search = GridSearchCV(crf, params_space, cv=3, verbose=1, n_jobs=-1, scoring=f1_scorer) # 5400 fits (in t<20minutes)\n",
    "search.fit(X_train, y_train)\n",
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996eeff",
   "metadata": {},
   "source": [
    "The best model has been selected and retrained on the entire training data (see the [documentation](https://scikit-learn.org/0.23/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV) for `RandomizedSearchCV`). Performance for the $F_1$ score is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb0926f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9700483654717622"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = search.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbccee31",
   "metadata": {},
   "source": [
    "Classification report below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca336315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                    O      0.983     0.989     0.986      1229\n",
      "B-DATUM_VERBUECHERUNG      1.000     1.000     1.000        15\n",
      "I-DATUM_VERBUECHERUNG      1.000     1.000     1.000        15\n",
      "      B-DATUM_VERTRAG      0.933     0.933     0.933        15\n",
      "      I-DATUM_VERTRAG      0.931     0.964     0.947        28\n",
      "            B-FLAECHE      0.944     1.000     0.971        17\n",
      "        B-GESAMTPREIS      1.000     0.591     0.743        22\n",
      "           B-IMMO_TYP      0.850     0.773     0.810        22\n",
      "           I-IMMO_TYP      0.000     0.000     0.000         1\n",
      "            B-KAEUFER      0.867     0.867     0.867        15\n",
      "            I-KAEUFER      0.909     0.952     0.930        21\n",
      "                B-ORT      0.957     0.918     0.938        49\n",
      "                I-ORT      1.000     0.929     0.963        14\n",
      "            B-QMPREIS      1.000     1.000     1.000        11\n",
      "            B-STRASSE      0.846     0.786     0.815        14\n",
      "            I-STRASSE      0.800     0.800     0.800         5\n",
      "   B-TERRASSENGROESSE      1.000     1.000     1.000         5\n",
      "         B-VERKAEUFER      0.833     0.833     0.833        12\n",
      "         I-VERKAEUFER      0.857     0.947     0.900        19\n",
      "\n",
      "             accuracy                          0.970      1529\n",
      "            macro avg      0.880     0.857     0.865      1529\n",
      "         weighted avg      0.972     0.970     0.970      1529\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthias/opt/anaconda3/envs/nlu/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=['O', 'B-DATUM_VERBUECHERUNG', 'I-DATUM_VERBUECHERUNG', 'B-DATUM_VERTRAG', 'I-DATUM_VERTRAG', 'B-FLAECHE', 'B-GESAMTPREIS', 'B-IMMO_TYP', 'I-IMMO_TYP', 'B-KAEUFER', 'I-KAEUFER', 'B-ORT', 'I-ORT', 'B-QMPREIS', 'B-STRASSE', 'I-STRASSE', 'B-TERRASSENGROESSE', 'B-VERKAEUFER', 'I-VERKAEUFER'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_test_labels = sorted(test_labels, key=lambda name: name[1:])\n",
    "print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_test_labels, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa5a06",
   "metadata": {},
   "source": [
    "$\\checkmark$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
