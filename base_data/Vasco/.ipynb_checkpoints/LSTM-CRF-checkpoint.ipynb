{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fd914f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "cde55b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.metrics import classification_report, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from torch_model_base import TorchModelBase\n",
    "#from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNDataset, TorchRNNClassifier, TorchRNNModel\n",
    "import utils\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "28f80a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "   # max_score = vec[0, argmax(vec)]\n",
    "    max_score = vec[0, torch.argmax(vec,axis=1)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def prepare_sequences(seqs, to_ix):\n",
    "    idxs = [prepare_sequence(seq, to_ix) for seq in seqs]\n",
    "    #return torch.tensor(idxs, dtype=torch.long)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88806e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                # best_tag_id = argmax(next_tag_var)\n",
    "                best_tag_id = torch.argmax(next_tag_var,axis=1)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        # best_tag_id = argmax(terminal_var)\n",
    "        best_tag_id = torch.argmax(terminal_var,axis=1)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "880e5116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([124.4205]), [tensor([4]), tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([10]), tensor([1]), tensor([4]), tensor([9]), tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([10]), tensor([1]), tensor([4]), tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([10]), tensor([1]), tensor([4]), tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([10]), tensor([1]), tensor([4])])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "(tensor([544.3170]), [tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([2]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([7]), tensor([7]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([9]), tensor([9]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([10]), tensor([10]), tensor([11]), tensor([11]), tensor([11])])\n"
     ]
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "tol = 1e-05\n",
    "n_iter_no_change = 10\n",
    "validation_fraction=0.1\n",
    "\n",
    "# Make up some training data\n",
    "#training_data = [(\n",
    "#    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "#    \"B I I I O O O B I O O\".split()\n",
    "#), (\n",
    "#    \"georgia tech is a university in georgia\".split(),\n",
    "#    \"B I O O O O B\".split()\n",
    "#)]\n",
    "\n",
    "#word_to_ix = {}\n",
    "#for sentence, tags in training_data:\n",
    "#    for word in sentence:\n",
    "#        if word not in word_to_ix:\n",
    "#            word_to_ix[word] = len(word_to_ix) # i.e. each successive word added gets a successive index\n",
    "\n",
    "#tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# Check predictions before training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    print(model(precheck_sent))\n",
    "\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(10):\n",
    "        #300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch)\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834c3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    }
   ],
   "source": [
    "print(precheck_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f1714b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DORNBIRN', 'In', 'der', 'Schulgasse', 'in', 'Dornbirn', 'hat', 'eine', '71,93', 'Quadratmeter', 'große', 'Wohnung', 'für', 'einen', 'Quadratmeterpreis', 'von', '5533,71', 'Euro', 'den', 'Besitzer', 'gewechselt', '.', 'Dieser', 'beinhaltet', 'auch', 'einen', 'Pkw-Abstellplatz', '.', 'Käufer', 'der', 'Wohnung', 'mit', '9,86', 'Quadratmetern', 'Terrasse', 'ist', 'die', 'ValLiLean', 'Beteiligungs-', 'und', 'Immobilienverwaltungs', 'GmbH', 'Beim', 'Verkäufer', 'handelt', 'es', 'sich', 'um', 'die', 'Karrenblick', 'Projekt', 'GmbH', ' ', 'Der', 'Kaufpreis', 'liegt', 'bei', '398.040', 'Euro', '.', 'Unterzeichnet', 'wurde', 'der', 'Kaufvertrag', 'am', '18.', 'September', '.', 'Die', 'Verbücherung', 'datiert', 'mit', 'Oktober', '2020', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68cb6ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, 'wall': 1, 'street': 2, 'journal': 3, 'reported': 4, 'today': 5, 'that': 6, 'apple': 7, 'corporation': 8, 'made': 9, 'money': 10, 'georgia': 11, 'tech': 12, 'is': 13, 'a': 14, 'university': 15, 'in': 16}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d8d0198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['the', 'wall', 'street', 'journal', 'reported', 'today', 'that', 'apple', 'corporation', 'made', 'money'], ['B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O']), (['georgia', 'tech', 'is', 'a', 'university', 'in', 'georgia'], ['B', 'I', 'O', 'O', 'O', 'O', 'B'])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2f9d2",
   "metadata": {},
   "source": [
    "Run with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fa057dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "1cb214c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN \n",
    "### NOTE: Make sure to copy most up-to-date annotations2.jsonl file to /Vasco/\n",
    "with open('annotations2.jsonl') as jsonl_file:\n",
    "    # note: after running data-preprocessing.ipynb this file already has token-level labels\n",
    "    lines = jsonl_file.readlines()\n",
    "annot = [json.loads(line) for line in lines]\n",
    "#print(annot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "643f2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN\n",
    "# now convert annotation tokens into list (sentences) of lists (tokens) format for sklearn_crfsuite.CRF\n",
    "train_sents=[] \n",
    "for j in range(0,len(annot)):\n",
    "    a = annot[j]['tokens']\n",
    "    # Only add sample if there are annotations\n",
    "    if annot[j]['spans']!=[]:\n",
    "        train_sentence = []\n",
    "        for i in range(0,len(a)):\n",
    "            if 'label' in a[i]: # only add element if this sample sentence has been labelled \n",
    "                token_element = (a[i]['text'],a[i]['label'])\n",
    "                train_sentence.append(token_element)\n",
    "        train_sents.append(train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "26637902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DORNBIRN', 'ORT'), ('In', 'O'), ('der', 'O'), ('Schulgasse', 'STRASSE'), ('in', 'O'), ('Dornbirn', 'ORT'), ('hat', 'O'), ('eine', 'O'), ('71,93', 'FLAECHE'), ('Quadratmeter', 'O'), ('große', 'O'), ('Wohnung', 'IMMO_TYP'), ('für', 'O'), ('einen', 'O'), ('Quadratmeterpreis', 'O'), ('von', 'O'), ('5533,71', 'QMPREIS'), ('Euro', 'O'), ('den', 'O'), ('Besitzer', 'O'), ('gewechselt', 'O'), ('.', 'O'), ('Dieser', 'O'), ('beinhaltet', 'O'), ('auch', 'O'), ('einen', 'O'), ('Pkw-Abstellplatz', 'O'), ('.', 'O'), ('Käufer', 'O'), ('der', 'O'), ('Wohnung', 'O'), ('mit', 'O'), ('9,86', 'TERRASSENGROESSE'), ('Quadratmetern', 'O'), ('Terrasse', 'O'), ('ist', 'O'), ('die', 'O'), ('ValLiLean', 'KAEUFER'), ('Beteiligungs-', 'KAEUFER'), ('und', 'KAEUFER'), ('Immobilienverwaltungs', 'KAEUFER'), ('GmbH', 'KAEUFER'), ('Beim', 'O'), ('Verkäufer', 'O'), ('handelt', 'O'), ('es', 'O'), ('sich', 'O'), ('um', 'O'), ('die', 'O'), ('Karrenblick', 'VERKAEUFER'), ('Projekt', 'VERKAEUFER'), ('GmbH', 'VERKAEUFER'), (' ', 'O'), ('Der', 'O'), ('Kaufpreis', 'O'), ('liegt', 'O'), ('bei', 'O'), ('398.040', 'GESAMTPREIS'), ('Euro', 'O'), ('.', 'O'), ('Unterzeichnet', 'O'), ('wurde', 'O'), ('der', 'O'), ('Kaufvertrag', 'O'), ('am', 'O'), ('18.', 'DATUM_VERTRAG'), ('September', 'DATUM_VERTRAG'), ('.', 'O'), ('Die', 'O'), ('Verbücherung', 'O'), ('datiert', 'O'), ('mit', 'O'), ('Oktober', 'DATUM_VERBUECHERUNG'), ('2020', 'DATUM_VERBUECHERUNG'), ('.', 'O'), ('.', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d6ffb67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into format expected above\n",
    "all_data = [([a1 for a1,a2 in el],[a2 for a1,a2 in el]) for el in train_sents]\n",
    "#tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "tag_to_ix = {'ORT': 0, 'STRASSE': 1, 'FLAECHE': 2, 'IMMO_TYP': 3, 'QMPREIS': 4, 'TERRASSENGROESSE': 5, 'KAEUFER': 6, 'VERKAEUFER': 7, 'GESAMTPREIS': 8, 'DATUM_VERTRAG': 9, 'DATUM_VERBUECHERUNG': 10, 'O': 11, START_TAG: 12, STOP_TAG: 13}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51dbe262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "for i in range(0,len(train_sents)):\n",
    "    if len(train_sents[i])==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8121791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "04b5b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sentence, tags in all_data: # i.e. before splitting training / test data\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix) # i.e. each successive word added gets a successive index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8cf27d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training / test.\n",
    "training_data=all_data[:100]\n",
    "test_data=all_data[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "239b2573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nenzing', ':', '50', 'ha']\n",
      "tensor([882, 385, 883, 878])\n",
      "['ORT', 'O', 'O', 'O']\n",
      "(tensor([14.7289], grad_fn=<IndexBackward>), [tensor([0]), tensor([11]), tensor([11]), tensor([11])])\n"
     ]
    }
   ],
   "source": [
    "# generate predictions v2\n",
    "\n",
    "sentences = [a for a,_ in test_data]\n",
    "labels = [b for _,b in test_data]\n",
    "print(sentences[0])\n",
    "print(prepare_sequence(sentences[0], word_to_ix))\n",
    "#test_data_preds = model(prepare_sequence(sentences[0], word_to_ix))\n",
    "#print(test_data_preds)\n",
    "print(labels[0])\n",
    "test_data_preds = [model(prepare_sequence(a, word_to_ix)) for a in sentences]\n",
    "print(test_data_preds[0])\n",
    "test_data_gold = [[tag_to_ix[el] for el in a] for a in labels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "7549419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now unfold both gold and preds list of tensors - not necessary for sklearn classification_report\n",
    "#test_data_preds_unfold = [int(item) for sublist in test_data_preds for item in sublist[1]]\n",
    "#test_data_gold_unfold = [int(item) for sublist in test_data_gold for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "39c7cf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'ORT', 1: 'STRASSE', 2: 'FLAECHE', 3: 'IMMO_TYP', 4: 'QMPREIS', 5: 'TERRASSENGROESSE', 6: 'KAEUFER', 7: 'VERKAEUFER', 8: 'GESAMTPREIS', 9: 'DATUM_VERTRAG', 10: 'DATUM_VERBUECHERUNG', 11: 'O', 12: '<START>', 13: '<STOP>'}\n"
     ]
    }
   ],
   "source": [
    "ix_to_tag = {value: key for key, value in tag_to_ix.items()}\n",
    "print(ix_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6824d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data_preds_unfold_tag = [str(ix_to_tag[el]) for el in test_data_preds_unfold]\n",
    "#test_data_gold_unfold_tag = [str(ix_to_tag[el]) for el in test_data_gold_unfold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3dc45220",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_distinct = list(tag_to_ix.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1ce9df14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'KAEUFER', 'DATUM_VERBUECHERUNG', 'DATUM_VERTRAG', 'VERKAEUFER', 'TERRASSENGROESSE', 'GESAMTPREIS', 'FLAECHE', 'IMMO_TYP', 'QMPREIS', 'ORT', '<START>', '<STOP>', 'STRASSE']\n"
     ]
    }
   ],
   "source": [
    "print(sorted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1df7c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0]), tensor([11]), tensor([11]), tensor([11])]\n"
     ]
    }
   ],
   "source": [
    "print(list(test_data_preds[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e220329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "61b59d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't unfold but convert ids to tags:\n",
    "test_data_preds_tag = [[str(ix_to_tag[int(item)]) for item in sublist[1]] for sublist in test_data_preds]\n",
    "test_data_gold_tag = [[str(ix_to_tag[int(item)]) for item in sublist] for sublist in test_data_gold ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "e4d09f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                ORT      0.730     0.540     0.621        50\n",
      "            STRASSE      0.000     0.000     0.000        19\n",
      "            FLAECHE      0.000     0.000     0.000        17\n",
      "           IMMO_TYP      0.000     0.000     0.000        22\n",
      "            QMPREIS      0.000     0.000     0.000        11\n",
      "   TERRASSENGROESSE      0.000     0.000     0.000         5\n",
      "            KAEUFER      0.000     0.000     0.000        22\n",
      "         VERKAEUFER      0.500     0.433     0.464        30\n",
      "        GESAMTPREIS      0.000     0.000     0.000        22\n",
      "      DATUM_VERTRAG      0.632     0.774     0.696        31\n",
      "DATUM_VERBUECHERUNG      0.913     0.724     0.808        29\n",
      "                  O      0.878     0.974     0.923      1107\n",
      "            <START>      0.000     0.000     0.000         0\n",
      "             <STOP>      0.000     0.000     0.000         0\n",
      "\n",
      "          micro avg      0.852     0.852     0.852      1365\n",
      "          macro avg      0.261     0.246     0.251      1365\n",
      "       weighted avg      0.783     0.852     0.815      1365\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasco\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\vasco\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted( # labels are coming in as id's not names\n",
    "    labels_distinct,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "\n",
    "print(metrics.flat_classification_report(\n",
    "    test_data_gold_tag, test_data_preds_tag, labels=labels_distinct, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2bb6e8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['DORNBIRN', 'In', 'der', 'Schulgasse', 'in', 'Dornbirn', 'hat', 'eine', '71,93', 'Quadratmeter', 'große', 'Wohnung', 'für', 'einen', 'Quadratmeterpreis', 'von', '5533,71', 'Euro', 'den', 'Besitzer', 'gewechselt', '.', 'Dieser', 'beinhaltet', 'auch', 'einen', 'Pkw-Abstellplatz', '.', 'Käufer', 'der', 'Wohnung', 'mit', '9,86', 'Quadratmetern', 'Terrasse', 'ist', 'die', 'ValLiLean', 'Beteiligungs-', 'und', 'Immobilienverwaltungs', 'GmbH', 'Beim', 'Verkäufer', 'handelt', 'es', 'sich', 'um', 'die', 'Karrenblick', 'Projekt', 'GmbH', ' ', 'Der', 'Kaufpreis', 'liegt', 'bei', '398.040', 'Euro', '.', 'Unterzeichnet', 'wurde', 'der', 'Kaufvertrag', 'am', '18.', 'September', '.', 'Die', 'Verbücherung', 'datiert', 'mit', 'Oktober', '2020', '.', '.', '.'], ['ORT', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "print(all_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "765c9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [el for auxList1,_ in all_data for el in auxList1]\n",
    "vocab = list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "4af6bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF_2(TorchRNNClassifier):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, **base_kwargs):\n",
    "        super(BiLSTM_CRF_2, self).__init__(tag_to_ix,embedding_dim,hidden_dim,**base_kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                # best_tag_id = argmax(next_tag_var)\n",
    "                best_tag_id = torch.argmax(next_tag_var,axis=1)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        # best_tag_id = argmax(terminal_var)\n",
    "        best_tag_id = torch.argmax(terminal_var,axis=1)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "41ed2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "3db7fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_mod = BiLSTM_CRF_2(\n",
    "    len(word_to_ix),\n",
    "    tag_to_ix,\n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    early_stopping=True,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "488836c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['DORNBIRN', 'In', 'der', 'Schulgasse', 'in', 'Dornbirn', 'hat', 'eine', '71,93', 'Quadratmeter', 'große', 'Wohnung', 'für', 'einen', 'Quadratmeterpreis', 'von', '5533,71', 'Euro', 'den', 'Besitzer', 'gewechselt', '.', 'Dieser', 'beinhaltet', 'auch', 'einen', 'Pkw-Abstellplatz', '.', 'Käufer', 'der', 'Wohnung', 'mit', '9,86', 'Quadratmetern', 'Terrasse', 'ist', 'die', 'ValLiLean', 'Beteiligungs-', 'und', 'Immobilienverwaltungs', 'GmbH', 'Beim', 'Verkäufer', 'handelt', 'es', 'sich', 'um', 'die', 'Karrenblick', 'Projekt', 'GmbH', ' ', 'Der', 'Kaufpreis', 'liegt', 'bei', '398.040', 'Euro', '.', 'Unterzeichnet', 'wurde', 'der', 'Kaufvertrag', 'am', '18.', 'September', '.', 'Die', 'Verbücherung', 'datiert', 'mit', 'Oktober', '2020', '.', '.', '.'], ['ORT', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "56ae09a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DORNBIRN', 'ORT'), ('In', 'O'), ('der', 'O'), ('Schulgasse', 'STRASSE'), ('in', 'O'), ('Dornbirn', 'ORT'), ('hat', 'O'), ('eine', 'O'), ('71,93', 'FLAECHE'), ('Quadratmeter', 'O'), ('große', 'O'), ('Wohnung', 'IMMO_TYP'), ('für', 'O'), ('einen', 'O'), ('Quadratmeterpreis', 'O'), ('von', 'O'), ('5533,71', 'QMPREIS'), ('Euro', 'O'), ('den', 'O'), ('Besitzer', 'O'), ('gewechselt', 'O'), ('.', 'O'), ('Dieser', 'O'), ('beinhaltet', 'O'), ('auch', 'O'), ('einen', 'O'), ('Pkw-Abstellplatz', 'O'), ('.', 'O'), ('Käufer', 'O'), ('der', 'O'), ('Wohnung', 'O'), ('mit', 'O'), ('9,86', 'TERRASSENGROESSE'), ('Quadratmetern', 'O'), ('Terrasse', 'O'), ('ist', 'O'), ('die', 'O'), ('ValLiLean', 'KAEUFER'), ('Beteiligungs-', 'KAEUFER'), ('und', 'KAEUFER'), ('Immobilienverwaltungs', 'KAEUFER'), ('GmbH', 'KAEUFER'), ('Beim', 'O'), ('Verkäufer', 'O'), ('handelt', 'O'), ('es', 'O'), ('sich', 'O'), ('um', 'O'), ('die', 'O'), ('Karrenblick', 'VERKAEUFER'), ('Projekt', 'VERKAEUFER'), ('GmbH', 'VERKAEUFER'), (' ', 'O'), ('Der', 'O'), ('Kaufpreis', 'O'), ('liegt', 'O'), ('bei', 'O'), ('398.040', 'GESAMTPREIS'), ('Euro', 'O'), ('.', 'O'), ('Unterzeichnet', 'O'), ('wurde', 'O'), ('der', 'O'), ('Kaufvertrag', 'O'), ('am', 'O'), ('18.', 'DATUM_VERTRAG'), ('September', 'DATUM_VERTRAG'), ('.', 'O'), ('Die', 'O'), ('Verbücherung', 'O'), ('datiert', 'O'), ('mit', 'O'), ('Oktober', 'DATUM_VERBUECHERUNG'), ('2020', 'DATUM_VERBUECHERUNG'), ('.', 'O'), ('.', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "f94eae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all = [[a for a,_ in el] for el in train_sents ], [[b for _,b in el] for el in train_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "fe1e4355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VR ADDED March 30th\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_ratio = 0.75\n",
    "train_test_split = round(0.75*len(X_all) - 0.5) # -0.5 => floor\n",
    "idx = [i for i in range(0,len(X_all))]\n",
    "idx_shuffle = shuffle(idx,random_state=0)\n",
    "X_shuffle, y_shuffle = [X_all[auxIdx] for auxIdx in idx_shuffle], [y_all[auxIdx] for auxIdx in idx_shuffle]\n",
    "\n",
    "X_train, X_test, y_train, y_test = X_shuffle[:train_test_split], X_shuffle[train_test_split:], y_shuffle[:train_test_split], y_shuffle[train_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9916c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = seq_mod.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
