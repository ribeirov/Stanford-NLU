{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "46bc54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.metrics import classification_report, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from torch_model_base import TorchModelBase\n",
    "#from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from torch_rnn_classifier import TorchRNNDataset, TorchRNNClassifier, TorchRNNModel\n",
    "import utils\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "df4daa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refresh torch rnn classifier:\n",
    "import importlib\n",
    "import torch_rnn_classifier\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_rnn_classifier import TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c519f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc5fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotations2.jsonl') as jsonl_file:\n",
    "    # note: after running data-preprocessing.ipynb this file already has token-level labels\n",
    "    lines = jsonl_file.readlines()\n",
    "annot = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62fbb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get data into format that TorchRNN expects:\n",
    "X=[] \n",
    "y=[]\n",
    "for j in range(0,len(annot)):\n",
    "    a = annot[j]['tokens']\n",
    "    auxX = []\n",
    "    auxy = []\n",
    "    if annot[j]['spans']!=[]: # are there annot for this example?\n",
    "        for i in range(0,len(a)):\n",
    "            #token_element = (a[i]['text'],a[i]['label'])\n",
    "            auxX.append(a[i]['text'])\n",
    "            auxy.append(a[i]['label'])\n",
    "        X.append(auxX)\n",
    "        y.append(auxy)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train, X_test, y_train, y_test = X[:120], X[120:], y[:120], y[120:]\n",
    "vocab = sorted({w for seq in X_train for w in seq}) + [\"$UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "b8b5c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload vsm module\n",
    "import torch_rnn_classifier, torch_model_base\n",
    "import importlib\n",
    "importlib.reload(torch_model_base)\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNModel, TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "a33711db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRNNSequenceLabeler(TorchRNNClassifier):\n",
    "\n",
    "    def build_graph(self): # uses this build_graph instead of TorchRNNClassifier.build_graph\n",
    "        print(\"here0\")\n",
    "        rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "        print(\"here02\")\n",
    "        model = TorchSequenceLabeler( # this defines self.model\n",
    "            rnn=rnn,\n",
    "            output_dim=self.n_classes_)\n",
    "        self.embed_dim = rnn.embed_dim\n",
    "        return model\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        START_TAG = \"<START>\"\n",
    "        STOP_TAG = \"<STOP>\"\n",
    "        X, seq_lengths = self._prepare_sequences(X) # converts tokens into tokenIds\n",
    "        if y is None:\n",
    "            return TorchRNNDataset(X, seq_lengths)\n",
    "        else:\n",
    "            # These are the changes from a regular classifier. All\n",
    "            # concern the fact that our labels are sequences of labels.\n",
    "            self.classes_ = sorted({x for seq in y for x in seq})\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            # `y` is a list of tensors of different length. Our Dataset\n",
    "            # class will turn it into a padding tensor for processing.\n",
    "            y = [torch.tensor([class2index[label] for label in seq])\n",
    "                 for seq in y] # converts labels to indices\n",
    "            return TorchRNNDataset(X, seq_lengths, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        # The base class does the heavy lifting:\n",
    "        preds = self._predict(X)\n",
    "        # Trim to the actual sequence lengths:\n",
    "        preds = [p[: l] for p, l in zip(preds, seq_lengths)]\n",
    "        # Use `softmax`; the model doesn't do this because the loss\n",
    "        # function does it internally.\n",
    "        probs = [torch.softmax(seq, dim=1) for seq in preds]\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X): #out: list of lists with text labels of predictions\n",
    "        probs = self.predict_proba(X)\n",
    "        return [[self.classes_[i] for i in seq.argmax(axis=1)] for seq in probs] # seq.argmax(axis=1) gives index of col that maximizes softmax prob\n",
    "        # see difference vs TorchRNNClassifier.predict\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        flat_preds = [x for seq in preds for x in seq]\n",
    "        flat_y = [x for seq in y for x in seq]\n",
    "        return utils.safe_macro_f1(flat_y, flat_preds)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class TorchSequenceLabeler(nn.Module): # no self.hidden_layer or self.classifier_activation as TorchRNNClassifierModel\n",
    "    def __init__(self, rnn, output_dim):\n",
    "        print(\"here021\")\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.output_dim = output_dim\n",
    "        if self.rnn.bidirectional:\n",
    "            self.classifier_dim = self.rnn.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.rnn.hidden_dim\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.classifier_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, X, seq_lengths): # X is (noExsInBatch,MaxLen)=(108,117), seq_lengths is the number of tokens in each example in each batch\n",
    "        # Out are logits - probs of each token for each class; logits are (108,117,12) or (1,11,5) = (batchSize,MaxLen of examples in batch,noLabelClasses) noLabelClasses include Start + End\n",
    "        # this is the forward method of self.model\n",
    "        print(\"here2\")\n",
    "        outputs, state = self.rnn(X, seq_lengths) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "       # print(\"out1\")\n",
    "        #print(state[0].data.shape)\n",
    "        #print(state[1].data.shape)\n",
    "        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "       # print(\"out2\")\n",
    "        print(outputs.data.shape)\n",
    "        #print(seq_length)\n",
    "        logits = self.classifier_layer(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "        # During training, we need to swap the dimensions of logits\n",
    "        # to accommodate `nn.CrossEntropyLoss`:\n",
    "        if self.training:\n",
    "            return logits.transpose(1, 2) # transpose dimensions 1 and 2 w/ each other (3d array) # outputs (108,12,117) or (1,5,11)\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4010d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\"the wall street journal reported today that apple corporation made money\".split(),\"georgia tech is a university in georgia\".split()]\n",
    "y_train = [\"B I I I O O O B I O O\".split(),\"B I O O O O B\".split()]\n",
    "vocab = sorted({w for seq in X_train for w in seq}) + [\"$UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "fe4012cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_mod = TorchRNNSequenceLabeler(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "12654d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "TorchSequenceLabeler(\n",
      "  (rnn): TorchRNNModel(\n",
      "    (embedding): Embedding(1049, 50)\n",
      "    (rnn): LSTM(50, 50, batch_first=True)\n",
      "  )\n",
      "  (classifier_layer): Linear(in_features=50, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "graph0 = seq_mod.build_graph()\n",
    "print(graph0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "f8f210b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 3 of 1000; error is 2.481832504272461"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here00\n",
      "here0\n",
      "here02\n",
      "here021\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.5070, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.4944, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.4818, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 7 of 1000; error is 2.4303898811340336"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.4692, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.4564, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.4435, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.4304, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 10 of 1000; error is 2.389024257659912"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.4170, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.4032, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.3890, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.3743, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 15 of 1000; error is 2.3074116706848145"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.3589, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.3427, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.3256, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.3074, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 19 of 1000; error is 2.2204887866973877"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.2880, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.2672, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.2448, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.2205, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 23. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 2.1011080741882324"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.1942, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.1656, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.1347, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "torch.Size([108, 92, 50])\n",
      "err\n",
      "tensor(2.1011, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "here2\n",
      "torch.Size([12, 117, 50])\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b9e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload vsm module\n",
    "import torch_rnn_classifier, torch_model_base\n",
    "import importlib\n",
    "importlib.reload(torch_model_base)\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNModel, TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ceee1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "\n",
    "class TorchCRFSequenceLabeler_1(TorchRNNClassifier):\n",
    "\n",
    "    def build_graph(self): # uses this build_graph instead of TorchRNNClassifier.build_graph\n",
    "        print(\"here0\")\n",
    "        rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "        print(\"here02\")\n",
    "        model = TorchSequenceLabeler_forCRF_1( # this defines self.model\n",
    "            rnn=rnn,\n",
    "            output_dim=self.n_classes_)\n",
    "        self.embed_dim = rnn.embed_dim\n",
    "        return model\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        START_TAG = \"<START>\"\n",
    "        STOP_TAG = \"<STOP>\"\n",
    "        X, seq_lengths = self._prepare_sequences(X) # converts tokens into tokenIds\n",
    "        if y is None:\n",
    "            return TorchRNNDataset(X, seq_lengths)\n",
    "        else:\n",
    "            # These are the changes from a regular classifier. All\n",
    "            # concern the fact that our labels are sequences of labels.\n",
    "            self.classes_ = sorted({x for seq in y for x in seq})\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            #class2index = dict(zip(self.classes_, range(2,2+self.n_classes_)))\n",
    "            #class2index[STOP_TAG]=0    # add start and stop tags (note: stop needs to be 0 as that is default for padding in collate_fn)\n",
    "            #class2index[START_TAG]=1 \n",
    "            # `y` is a list of tensors of different length. Our Dataset\n",
    "            # class will turn it into a padding tensor for processing.\n",
    "            y = [torch.tensor([class2index[label] for label in seq])\n",
    "                 for seq in y] # converts labels to indices\n",
    "            return TorchRNNDataset(X, seq_lengths, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        # The base class does the heavy lifting:\n",
    "        preds = self._predict(X)\n",
    "        # Trim to the actual sequence lengths:\n",
    "        preds = [p[: l] for p, l in zip(preds, seq_lengths)]\n",
    "        # Use `softmax`; the model doesn't do this because the loss\n",
    "        # function does it internally.\n",
    "        probs = [torch.softmax(seq, dim=1) for seq in preds]\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return [[self.classes_[i] for i in seq.argmax(axis=1)] for seq in probs] # seq.argmax(axis=1) gives index of col that maximizes softmax prob\n",
    "        # see difference vs TorchRNNClassifier.predict\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        flat_preds = [x for seq in preds for x in seq]\n",
    "        flat_y = [x for seq in y for x in seq]\n",
    "        return utils.safe_macro_f1(flat_y, flat_preds)\n",
    "    \n",
    "    def nClasses(self):\n",
    "        return len(self.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a7e7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSequenceLabeler_forCRF_1(nn.Module): # no self.hidden_layer or self.classifier_activation as TorchRNNClassifierModel\n",
    "    def __init__(self, rnn, output_dim):\n",
    "        print(\"here021\")\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.output_dim = output_dim\n",
    "        if self.rnn.bidirectional:\n",
    "            self.classifier_dim = self.rnn.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.rnn.hidden_dim\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.classifier_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, X, seq_lengths): # X is (noExsInBatch,MaxLen)=(108,117), seq_lengths is the number of tokens in each example in each batch\n",
    "        # this is the forward method of self.model\n",
    "        print(\"here2\")\n",
    "        outputs, state = self.rnn(X, seq_lengths) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "        logits = self.classifier_layer(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "        # logits are (108,117,12) or (1,11,5) = (batchSize,MaxLen of examples in batch,noLabelClasses) noLabelClasses include Start + End\n",
    "        # During training, we need to swap the dimensions of logits\n",
    "        # to accommodate `nn.CrossEntropyLoss`:\n",
    "        if self.training:\n",
    "            return logits.transpose(1, 2) # transpose dimensions 1 and 2 w/ each other (3d array) # outputs (108,12,117) or (1,5,11)\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "16d1170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload vsm module\n",
    "import torch_rnn_classifier, torch_model_base\n",
    "import importlib\n",
    "importlib.reload(torch_model_base)\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNModel, TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ec364f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "\n",
    "class TorchCRFSequenceLabeler_2(TorchRNNClassifier):\n",
    "\n",
    "    def build_graph(self): # uses this build_graph instead of TorchRNNClassifier.build_graph\n",
    "        print(\"here0\")\n",
    "        rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "        self.crf = CRF(self.n_classes_,batch_first=True)\n",
    "        print(\"here02\")\n",
    "        model = TorchSequenceLabeler_forCRF_2( # this defines self.model\n",
    "            rnn=rnn,\n",
    "            output_dim=self.n_classes_,\n",
    "            crf=self.crf)\n",
    "        print(\"here002\")\n",
    "        self.embed_dim = rnn.embed_dim\n",
    "        return model\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        X, seq_lengths = self._prepare_sequences(X) # converts tokens into tokenIds\n",
    "        if y is None:\n",
    "            return TorchRNNDataset(X, seq_lengths)\n",
    "        else:\n",
    "            # These are the changes from a regular classifier. All\n",
    "            # concern the fact that our labels are sequences of labels.\n",
    "            self.classes_ = sorted({x for seq in y for x in seq})\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            #class2index = dict(zip(self.classes_, range(2,2+self.n_classes_)))\n",
    "            #class2index[STOP_TAG]=0    # add start and stop tags (note: stop needs to be 0 as that is default for padding in collate_fn)\n",
    "            #class2index[START_TAG]=1 \n",
    "            # `y` is a list of tensors of different length. Our Dataset\n",
    "            # class will turn it into a padding tensor for processing.\n",
    "            y = [torch.tensor([class2index[label] for label in seq])\n",
    "                 for seq in y] # converts labels to indices\n",
    "            return TorchRNNDataset(X, seq_lengths, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        # The base class does the heavy lifting:\n",
    "        preds = self._predict(X)\n",
    "        # Trim to the actual sequence lengths:\n",
    "        preds = [p[: l] for p, l in zip(preds, seq_lengths)]\n",
    "        # Use `softmax`; the model doesn't do this because the loss\n",
    "        # function does it internally.\n",
    "        probs = [torch.softmax(seq, dim=1) for seq in preds]\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X): # for CRF-RNN X are logits from RNN\n",
    "       # probs = self.predict_proba(X)\n",
    "       # return [[self.classes_[i] for i in seq.argmax(axis=1)] for seq in probs] # seq.argmax(axis=1) gives index of col that maximizes softmax prob\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        preds = self._predict(X)\n",
    "        # Trim to the actual sequence lengths:\n",
    "        preds = [p[: l] for p, l in zip(preds, seq_lengths)]        \n",
    "        mask=self.create_mask(seq_lengths) # creates mask matrix (1s are obs used in CRF; 0s are discarded)  \n",
    "        print(\"pred\")\n",
    "        print(X.shape)\n",
    "        print(mask.shape)\n",
    "       # tag_seq = self.crf.decode(X,mask=mask) # note: X is (nExs,maxTokLen) and here input must be (nExs,maxTokLen,nDistinctTags)\n",
    "        # [[self.classes_[i] for i in seq] for seq in tag_seq]\n",
    "        return 0\n",
    "        # see difference vs TorchRNNClassifier.predict\n",
    "\n",
    "    def score(self, X, y):\n",
    "       # preds = self.predict(X)\n",
    "       # flat_preds = [x for seq in preds for x in seq]\n",
    "       # flat_y = [x for seq in y for x in seq]\n",
    "       # return utils.safe_macro_f1(flat_y, flat_preds)\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        mask=self.create_mask(seq_lengths) # creates mask matrix (1s are obs used in CRF; 0s are discarded)\n",
    "        return self.crf(logits, y, mask=mask) # no negative sign here as we want to max likelihood     \n",
    "    \n",
    "    def nClasses(self):\n",
    "        return len(self.classes_)\n",
    "    \n",
    "    def create_mask(self, seq_length):\n",
    "        maxLen=max(seq_length)\n",
    "        auxLen=len(seq_length)\n",
    "        auxOne = torch.ones(maxLen)\n",
    "        auxZero = torch.zeros(maxLen)\n",
    "        auxOne_l=[1]*maxLen\n",
    "        auxZero_l=[0]*maxLen\n",
    "        auxMatrix=[]\n",
    "        for i in range(auxLen):\n",
    "            auxRow=auxOne_l[:seq_length[i]]+auxZero_l[seq_length[i]:]\n",
    "            auxMatrix.append(auxRow)\n",
    "        return torch.tensor(auxMatrix,dtype=torch.uint8)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "550fe65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSequenceLabeler_forCRF_2(nn.Module): # no self.hidden_layer or self.classifier_activation as TorchRNNClassifierModel\n",
    "    def __init__(self, rnn, output_dim, crf):\n",
    "        print(\"here021\")\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.output_dim = output_dim\n",
    "        if self.rnn.bidirectional:\n",
    "            self.classifier_dim = self.rnn.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.rnn.hidden_dim\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.classifier_dim, self.output_dim)\n",
    "        self.crf = crf\n",
    "\n",
    "    def forward(self, X, seq_lengths): # X is (noExsInBatch,MaxLen)=(108,117), seq_lengths is the number of tokens in each example in each batch\n",
    "        # this is the forward method of self.model\n",
    "        print(\"here2\")\n",
    "        outputs, state = self.rnn(X, seq_lengths) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "        logits = self.classifier_layer(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "       # print(logits.shape)\n",
    "        # logits are (108,117,12) or (1,11,5) = (batchSize,MaxLen of examples in batch,noLabelClasses) noLabelClasses include Start + End\n",
    "        # During training, we need to swap the dimensions of logits\n",
    "        # to accommodate `nn.CrossEntropyLoss`:\n",
    "       # if self.training:\n",
    "       #     return logits.transpose(1, 2) # transpose dimensions 1 and 2 w/ each other (3d array) # outputs (108,12,117) or (1,5,11)\n",
    "       # else:\n",
    "       #     return logits\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        mask = (self.create_mask(seq_lengths)).to(device, non_blocking=True)\n",
    "        print(\"fwd\")\n",
    "        print(logits.shape)\n",
    "        print(mask.shape)\n",
    "        #tag_seqs = self.crf.decode(logits, mask=mask) # most likely tag sequences\n",
    "        return self.crf.forward(logits, mask=mask)\n",
    "    \n",
    "    def create_mask(self, seq_length):\n",
    "        maxLen=max(seq_length)\n",
    "        auxLen=len(seq_length)\n",
    "        auxOne = torch.ones(maxLen)\n",
    "        auxZero = torch.zeros(maxLen)\n",
    "        auxOne_l=[1]*maxLen\n",
    "        auxZero_l=[0]*maxLen\n",
    "        auxMatrix=[]\n",
    "        for i in range(auxLen):\n",
    "            auxRow=auxOne_l[:seq_length[i]]+auxZero_l[seq_length[i]:]\n",
    "            auxMatrix.append(auxRow)\n",
    "        return torch.tensor(auxMatrix,dtype=torch.uint8)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f6014b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following converts words to indices and pads sequences\n",
    "seq_mod1 = TorchCRFSequenceLabeler_1(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff142e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = seq_mod1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2cb2bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(seq_length):\n",
    "    maxLen=max(seq_length)\n",
    "    auxLen=len(seq_length)\n",
    "    auxOne = torch.ones(maxLen)\n",
    "    auxZero = torch.zeros(maxLen)\n",
    "    auxOne_l=[1]*maxLen\n",
    "    auxZero_l=[0]*maxLen\n",
    "    auxMatrix=[]\n",
    "    for i in range(auxLen):\n",
    "        auxRow=auxOne_l[:seq_length[i]]+auxZero_l[seq_length[i]:]\n",
    "        auxMatrix.append(auxRow)\n",
    "    return torch.tensor(auxMatrix,dtype=torch.uint8)\n",
    "#check:\n",
    "#out=create_mask(seq_length)\n",
    "#print(out)\n",
    "#print(torch.sum(out,dim=1))\n",
    "\n",
    "#Note: before was doing this but can't use this as not guaranteed that label[STOP]=0\n",
    "    # a=torch.full_like(y,0,dtype=torch.uint8)\n",
    "    # b=torch.full_like(y,1,dtype=torch.uint8)\n",
    "    # mask=torch.where(y==0,a,b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "adf0ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "here2\n",
      "here21\n",
      "torch.Size([120, 117, 12])\n",
      "tensor(16929.6426, grad_fn=<NegBackward>)\n",
      "[[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]]\n"
     ]
    }
   ],
   "source": [
    "########## SAMPLE CRF ON REAL DATA\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "dataset = seq_mod1.build_dataset(X_train, y_train) \n",
    "dataloader = seq_mod1._build_dataloader(dataset, shuffle=False) \n",
    "graph = seq_mod1.build_graph()\n",
    "num_tags = seq_mod1.nClasses()\n",
    "model_CRF = CRF(num_tags,batch_first=True)\n",
    "\n",
    "optimizer = optim.SGD(model_CRF.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "for batch_num, batch in enumerate(dataloader, start=1):\n",
    "    x=batch[0]   \n",
    "    seq_length=batch[1]\n",
    "    y=batch[2] \n",
    "    logits = graph.forward(x,seq_length).transpose(1,2)\n",
    "    print(logits.shape)\n",
    "    \n",
    "    # CRF piece:\n",
    "    mask=create_mask(seq_length) # creates mask matrix (1s are obs used in CRF; 0s are discarded)\n",
    "    loss=-model_CRF(logits, y, mask=mask) # in this way minimizing loss means max log likelihood (i.e. we're converting to NLL)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(model_CRF.decode(logits,mask=mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ec4c4565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchSequenceLabeler_forCRF_1(\n",
      "  (rnn): TorchRNNModel(\n",
      "    (embedding): Embedding(1049, 50)\n",
      "    (rnn): LSTM(50, 50, batch_first=True)\n",
      "  )\n",
      "  (classifier_layer): Linear(in_features=50, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "61fae381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following converts words to indices and pads sequences\n",
    "seq_mod2 = TorchCRFSequenceLabeler_2(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "78c1e1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here00\n",
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "here002\n",
      "batch1\n",
      "here-model\n",
      "here2\n",
      "here21\n",
      "fwd\n",
      "torch.Size([108, 117, 12])\n",
      "torch.Size([108, 117])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\DeepLearning-JN\\cs224u\\base_data\\Vasco\\torch_model_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"here-model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m                 \u001b[0mbatch_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m                \u001b[1;31m# print(\"batch_preds\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m                 \u001b[1;31m#print(len(batch_preds))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2540/1238705126.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, seq_lengths)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#tag_seqs = self.crf.decode(logits, mask=mask) # most likely tag sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tags'"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "84edc576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "here002\n",
      "here2\n",
      "here21\n",
      "fwd\n",
      "torch.Size([120, 117, 12])\n",
      "torch.Size([120, 117])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2540/100546928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[1;31m#  print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[1;31m#  print(seq_length.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m    \u001b[1;31m# print(logits)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_mod2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2540/1238705126.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, seq_lengths)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#tag_seqs = self.crf.decode(logits, mask=mask) # most likely tag sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tags'"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "################### I AM WORKING HERE #######################\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "dataset = seq_mod2.build_dataset(X_train, y_train) \n",
    "dataloader = seq_mod2._build_dataloader(dataset, shuffle=False) \n",
    "graph = seq_mod2.build_graph()\n",
    "num_tags = seq_mod2.nClasses()\n",
    "#model_CRF = CRF(num_tags,batch_first=True)\n",
    "\n",
    "#optimizer = optim.SGD(model_CRF.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "for batch_num, batch in enumerate(dataloader, start=1):\n",
    "    x=batch[0]   \n",
    "    seq_length=batch[1]\n",
    "    y=batch[2] \n",
    "  #  print(x.shape)\n",
    "  #  print(seq_length.shape)\n",
    "    logits = graph.forward(x,seq_length)\n",
    "   # print(logits)\n",
    "    print(seq_mod2.predict(x))\n",
    "    \n",
    "    # CRF piece:\n",
    "   # mask=create_mask(seq_length) # creates mask matrix (1s are obs used in CRF; 0s are discarded)\n",
    "   # loss=-model_CRF(logits, y, mask=mask) # in this way minimizing loss means max log likelihood (i.e. we're converting to NLL)\n",
    "   # print(loss)\n",
    "   # loss.backward()\n",
    "   # optimizer.step()\n",
    "   # print(model_CRF.decode(logits,mask=mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8fa6a516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here0\n",
      "here01\n",
      "here02\n",
      "here021\n",
      "here002\n",
      "TorchSequenceLabeler_forCRF_2(\n",
      "  (rnn): TorchRNNModel(\n",
      "    (embedding): Embedding(1049, 50)\n",
      "    (rnn): LSTM(50, 50, batch_first=True)\n",
      "  )\n",
      "  (classifier_layer): Linear(in_features=50, out_features=12, bias=True)\n",
      "  (crf): CRF(num_tags=12)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "graph = seq_mod2.build_graph()\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1146153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "a05f1370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload vsm module\n",
    "import torch_rnn_classifier, torch_model_base\n",
    "import importlib\n",
    "importlib.reload(torch_model_base)\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNModel, TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "bf988313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "\n",
    "class TorchCRFSequenceLabeler_3(TorchRNNClassifier):\n",
    "\n",
    "    def __init__(self,             \n",
    "            vocab,\n",
    "            hidden_dim=50,\n",
    "            embedding=None,\n",
    "            use_embedding=True,\n",
    "            embed_dim=50,\n",
    "            rnn_cell_class=nn.LSTM,\n",
    "            bidirectional=False,\n",
    "            freeze_embedding=False,\n",
    "            classifier_activation=nn.ReLU(),\n",
    "            **base_kwargs):   \n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = embedding\n",
    "        self.use_embedding = use_embedding\n",
    "        self.embed_dim = embed_dim\n",
    "        self.rnn_cell_class = rnn_cell_class\n",
    "        self.bidirectional = bidirectional\n",
    "        self.freeze_embedding = freeze_embedding\n",
    "        self.classifier_activation = classifier_activation\n",
    "        super().__init__(vocab,**base_kwargs)\n",
    "        self.params += [\n",
    "            'hidden_dim',\n",
    "            'embed_dim',\n",
    "            'embedding',\n",
    "            'use_embedding',\n",
    "            'rnn_cell_class',\n",
    "            'bidirectional',\n",
    "            'freeze_embedding',\n",
    "            'classifier_activation']\n",
    "        self.loss = lambda x:x\n",
    "        if self.bidirectional:\n",
    "            self.classifier_dim = self.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.hidden_dim\n",
    "       # self.classifier_layer = nn.Linear(\n",
    "       #     self.classifier_dim, self.n_classes_)\n",
    "\n",
    "       \n",
    "    def build_graph(self): # uses this build_graph instead of TorchRNNClassifier.build_graph\n",
    "       # print(\"here0\")\n",
    "        rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "      #  print(\"here02\")\n",
    "        model = TorchSequenceLabeler_forCRF_3( # this defines self.model\n",
    "            rnn=rnn,\n",
    "            output_dim=self.n_classes_)\n",
    "      #  print(\"here002\")\n",
    "        self.embed_dim = rnn.embed_dim\n",
    "        self.rnn = rnn\n",
    "        return model\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        X, seq_lengths = self._prepare_sequences(X) # converts tokens into tokenIds\n",
    "        if y is None:\n",
    "            return TorchRNNDataset(X, seq_lengths)\n",
    "        else:\n",
    "            # These are the changes from a regular classifier. All\n",
    "            # concern the fact that our labels are sequences of labels.\n",
    "            self.classes_ = sorted({x for seq in y for x in seq})\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            print(class2index)\n",
    "            #class2index = dict(zip(self.classes_, range(2,2+self.n_classes_)))\n",
    "            #class2index[STOP_TAG]=0    # add start and stop tags (note: stop needs to be 0 as that is default for padding in collate_fn)\n",
    "            #class2index[START_TAG]=1 \n",
    "            # `y` is a list of tensors of different length. Our Dataset\n",
    "            # class will turn it into a padding tensor for processing.\n",
    "            y = [torch.tensor([class2index[label] for label in seq])\n",
    "                 for seq in y] # converts labels to indices\n",
    "            return TorchRNNDataset(X, seq_lengths, y)\n",
    "\n",
    "#    def predict_proba(self, X):\n",
    "#        seq_lengths = [len(ex) for ex in X]\n",
    "#        # The base class does the heavy lifting:\n",
    "#        preds = self._predict(X)\n",
    "#        # Trim to the actual sequence lengths:\n",
    "#        preds = [p[: l] for p, l in zip(preds, seq_lengths)]\n",
    "#        # Use `softmax`; the model doesn't do this because the loss\n",
    "#        # function does it internally.\n",
    "#        probs = [torch.softmax(seq, dim=1) for seq in preds]\n",
    "#        return probs\n",
    "\n",
    "    def predict(self, X): # for CRF-RNN X are logits from RNN\n",
    "       # probs = self.predict_proba(X)\n",
    "       # return [[self.classes_[i] for i in seq.argmax(axis=1)] for seq in probs] # seq.argmax(axis=1) gives index of col that maximizes softmax prob\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        preds = self._predict(X)     \n",
    "        mask=self.create_mask(seq_lengths).to(device, non_blocking=True) # creates mask matrix (1s are obs used in CRF; 0s are discarded)  \n",
    "        tag_seq = self.crf.decode(preds,mask=mask) # note: X is (nExs,maxTokLen) and here input must be (nExs,maxTokLen,nDistinctTags); out is optimal seq of tagIds\n",
    "        return [[self.classes_[i] for i in seq] for seq in tag_seq] \n",
    "        # see difference vs TorchRNNClassifier.predict\n",
    "\n",
    "#    def score(self, X, y):\n",
    "#       # preds = self.predict(X)\n",
    "#       # flat_preds = [x for seq in preds for x in seq]\n",
    "#       # flat_y = [x for seq in y for x in seq]\n",
    "#       # return utils.safe_macro_f1(flat_y, flat_preds)\n",
    "#        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#        seq_lengths = [len(ex) for ex in X]\n",
    "#        mask=self.create_mask(seq_lengths).to(device, non_blocking=True) # creates mask matrix (1s are obs used in CRF; 0s are discarded)\n",
    "#        outputs, state = self.rnn(X, torch.tensor(seq_lengths)) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "#        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "#        fcl = nn.Linear(self.classifier_dim, self.n_classes_).to(device, non_blocking=True)\n",
    "#        logits = fcl(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "#        # score\n",
    "#        print(self.crf(logits, y, mask=mask))\n",
    "#        return self.crf(logits, y, mask=mask)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        flat_preds = [x for seq in preds for x in seq]\n",
    "        flat_y = [x for seq in y for x in seq]\n",
    "        return utils.safe_macro_f1(flat_y, flat_preds)  \n",
    "    \n",
    "    def nClasses(self):\n",
    "        return len(self.classes_)\n",
    "    \n",
    "    def classes(self):\n",
    "        return self.classes_\n",
    "    \n",
    "    def create_mask(self, seq_length):\n",
    "        maxLen=max(seq_length)\n",
    "        auxLen=len(seq_length)\n",
    "        auxOne = torch.ones(maxLen)\n",
    "        auxZero = torch.zeros(maxLen)\n",
    "        auxOne_l=[1]*maxLen\n",
    "        auxZero_l=[0]*maxLen\n",
    "        auxMatrix=[]\n",
    "        for i in range(auxLen):\n",
    "            auxRow=auxOne_l[:seq_length[i]]+auxZero_l[seq_length[i]:]\n",
    "            auxMatrix.append(auxRow)\n",
    "        return torch.tensor(auxMatrix,dtype=torch.uint8)  \n",
    "\n",
    "    \n",
    "    def fit(self, *args):\n",
    "        \"\"\"\n",
    "        Generic optimization method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        *args: list of objects\n",
    "            We assume that the final element of args give the labels\n",
    "            and all the preceding elements give the system inputs.\n",
    "            For regular supervised learning, this is like (X, y), but\n",
    "            we allow for models that might use multiple data structures\n",
    "            for their inputs.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        model: nn.Module or subclass thereof\n",
    "            Set by `build_graph`. If `warm_start=True`, then this is\n",
    "            initialized only by the first call to `fit`.\n",
    "\n",
    "        optimizer: torch.optimizer.Optimizer\n",
    "            Set by `build_optimizer`. If `warm_start=True`, then this is\n",
    "            initialized only by the first call to `fit`.\n",
    "\n",
    "        errors: list of float\n",
    "            List of errors. If `warm_start=True`, then this is\n",
    "            initialized only by the first call to `fit`. Thus, where\n",
    "            `max_iter=5`, if we call `fit` twice with `warm_start=True`,\n",
    "            then `errors` will end up with 10 floats in it.\n",
    "\n",
    "        validation_scores: list\n",
    "            List of scores. This is filled only if `early_stopping=True`.\n",
    "            If `warm_start=True`, then this is initialized only by the\n",
    "            first call to `fit`. Thus, where `max_iter=5`, if we call\n",
    "            `fit` twice with `warm_start=True`, then `validation_scores`\n",
    "            will end up with 10 floats in it.\n",
    "\n",
    "        no_improvement_count: int\n",
    "            Used to control early stopping and convergence. These values\n",
    "            are controlled by `_update_no_improvement_count_early_stopping`\n",
    "            or `_update_no_improvement_count_errors`.  If `warm_start=True`,\n",
    "            then this is initialized only by the first call to `fit`. Thus,\n",
    "            in that situation, the values could accumulate across calls to\n",
    "            `fit`.\n",
    "\n",
    "        best_error: float\n",
    "           Used to control convergence. Smaller is assumed to be better.\n",
    "           If `warm_start=True`, then this is initialized only by the first\n",
    "           call to `fit`. It will be reset by\n",
    "           `_update_no_improvement_count_errors` depending on how the\n",
    "           optimization is proceeding.\n",
    "\n",
    "        best_score: float\n",
    "           Used to control early stopping. If `warm_start=True`, then this\n",
    "           is initialized only by the first call to `fit`. It will be reset\n",
    "           by `_update_no_improvement_count_early_stopping` depending on how\n",
    "           the optimization is proceeding. Important: we currently assume\n",
    "           that larger scores are better. As a result, we will not get the\n",
    "           correct results for, e.g., a scoring function based in\n",
    "           `mean_squared_error`. See `self.score` for additional details.\n",
    "\n",
    "        best_parameters: dict\n",
    "            This is a PyTorch state dict. It is used if and only if\n",
    "            `early_stopping=True`. In that case, it is updated whenever\n",
    "            `best_score` is improved numerically. If the early stopping\n",
    "            criteria are met, then `self.model` is reset to contain these\n",
    "            parameters before `fit` exits.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "      #  print(\"here00\")\n",
    "        if self.early_stopping:\n",
    "            args, dev = self._build_validation_split(\n",
    "                *args, validation_fraction=self.validation_fraction)\n",
    "            \n",
    "\n",
    "        # Dataset:\n",
    "        dataset = self.build_dataset(*args)\n",
    "        dataloader = self._build_dataloader(dataset, shuffle=True)\n",
    "\n",
    "        # Graph:\n",
    "        if not self.warm_start or not hasattr(self, \"model\"):\n",
    "            self.model = self.build_graph()\n",
    "            # This device move has to happen before the optimizer is built:\n",
    "            # https://pytorch.org/docs/master/optim.html#constructing-it\n",
    "            self.model.to(self.device)\n",
    "            self.optimizer = self.build_optimizer()\n",
    "            self.errors = []\n",
    "            self.validation_scores = []\n",
    "            self.no_improvement_count = 0\n",
    "            self.best_error = np.inf\n",
    "            self.best_score = -np.inf\n",
    "            self.best_parameters = None\n",
    "\n",
    "        # Make sure the model is where we want it:\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        self.crf = CRF(self.n_classes_,batch_first=True).to(self.device, non_blocking=True)\n",
    "\n",
    "        for iteration in range(1, self.max_iter+1):\n",
    "\n",
    "            epoch_error = 0.0\n",
    "\n",
    "            for batch_num, batch in enumerate(dataloader, start=1):\n",
    "               # print(\"batch\"+str(batch_num)) \n",
    "\n",
    "               # print(batch)\n",
    "                batch = [x.to(self.device, non_blocking=True) for x in batch]\n",
    "\n",
    "                X_batch = batch[: -1] # list w/ 2 els: 1st el is tensor (108xmaxLen) w/ tokens for each example in batch; 2nd el is (108x1) with lengths of each example\n",
    "                y_batch = batch[-1] # list with each element of this batch (108 el in list) with tensor (maxLen x 1) labels converted to ints and w/ len = maxLen of all example sequences # print(y_batch[0].shape)\n",
    "               # print(X_batch[1].shape)\n",
    "               # print(y_batch[0])\n",
    "               \n",
    "                batch_preds = self.model(*X_batch) # produces logits outputs of lstm\n",
    "               # print(\"batch_preds\")\n",
    "\n",
    "               # print(\"here-model2\")\n",
    "                mask = (self.create_mask(X_batch[1])).to(self.device, non_blocking=True)\n",
    "                #err = self.loss(batch_preds, y_batch) # batch_preds = (108,12,117); y_batch = (108,117)\n",
    "                err = -self.crf(batch_preds,y_batch,mask=mask,reduction='mean') \n",
    "                # NOTE: self.crf outputs log likelihood so we multiply by (-1) so as to minimize this result\n",
    "\n",
    "                if self.gradient_accumulation_steps > 1 and \\\n",
    "                  self.loss.reduction == \"mean\":\n",
    "                    err /= self.gradient_accumulation_steps\n",
    "\n",
    "                err.backward()\n",
    "\n",
    "                epoch_error += err.item()\n",
    "\n",
    "                if batch_num % self.gradient_accumulation_steps == 0 or \\\n",
    "                  batch_num == len(dataloader):\n",
    "                    if self.max_grad_norm is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), self.max_grad_norm)\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "            # Stopping criteria:\n",
    "\n",
    "            if self.early_stopping:\n",
    "                self._update_no_improvement_count_early_stopping(*dev) # here we max macro avg f1 score (on dev = validation set)\n",
    "                if self.no_improvement_count > self.n_iter_no_change:\n",
    "                    utils.progress_bar(\n",
    "                        \"Stopping after epoch {}. Validation score did \"\n",
    "                        \"not improve by tol={} for more than {} epochs. \"\n",
    "                        \"Final error is {}\".format(iteration, self.tol,\n",
    "                            self.n_iter_no_change, epoch_error),\n",
    "                        verbose=self.display_progress)\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                self._update_no_improvement_count_errors(epoch_error)\n",
    "                if self.no_improvement_count > self.n_iter_no_change:\n",
    "                    utils.progress_bar(\n",
    "                        \"Stopping after epoch {}. Training loss did \"\n",
    "                        \"not improve more than tol={}. Final error \"\n",
    "                        \"is {}.\".format(iteration, self.tol, epoch_error),\n",
    "                        verbose=self.display_progress)\n",
    "                    break\n",
    "\n",
    "            utils.progress_bar(\n",
    "                \"Finished epoch {} of {}; error is {}\".format(\n",
    "                    iteration, self.max_iter, epoch_error),\n",
    "                verbose=self.display_progress)\n",
    "\n",
    "        if self.early_stopping:\n",
    "            self.model.load_state_dict(self.best_parameters)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "448e95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSequenceLabeler_forCRF_3(nn.Module): # no self.hidden_layer or self.classifier_activation as TorchRNNClassifierModel\n",
    "    def __init__(self, rnn, output_dim):\n",
    "       # print(\"here021\")\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.output_dim = output_dim\n",
    "        if self.rnn.bidirectional:\n",
    "            self.classifier_dim = self.rnn.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.rnn.hidden_dim\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.classifier_dim, self.output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, X, seq_lengths): # X is (noExsInBatch,MaxLen)=(108,117), seq_lengths is the number of tokens in each example in each batch\n",
    "        # this is the forward method of self.model\n",
    "       # print(\"here2\")\n",
    "        outputs, state = self.rnn(X, seq_lengths) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "        logits = self.classifier_layer(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "       # print(logits.shape)\n",
    "        # logits are (108,117,12) or (1,11,5) = (batchSize,MaxLen of examples in batch,noLabelClasses) noLabelClasses include Start + End\n",
    "        return logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "1c8ad2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following converts words to indices and pads sequences\n",
    "seq_mod3 = TorchCRFSequenceLabeler_3(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "7c6cf13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATUM_VERBUECHERUNG': 0, 'DATUM_VERTRAG': 1, 'FLAECHE': 2, 'GESAMTPREIS': 3, 'IMMO_TYP': 4, 'KAEUFER': 5, 'O': 6, 'ORT': 7, 'QMPREIS': 8, 'STRASSE': 9, 'TERRASSENGROESSE': 10, 'VERKAEUFER': 11}\n",
      "TorchSequenceLabeler_forCRF_3(\n",
      "  (rnn): TorchRNNModel(\n",
      "    (embedding): Embedding(1049, 50)\n",
      "    (rnn): LSTM(50, 50, batch_first=True)\n",
      "  )\n",
      "  (classifier_layer): Linear(in_features=50, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dataset = seq_mod3.build_dataset(X_train, y_train) \n",
    "graph = seq_mod3.build_graph()\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "159121d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATUM_VERBUECHERUNG': 0, 'DATUM_VERTRAG': 1, 'FLAECHE': 2, 'GESAMTPREIS': 3, 'IMMO_TYP': 4, 'KAEUFER': 5, 'O': 6, 'ORT': 7, 'QMPREIS': 8, 'STRASSE': 9, 'TERRASSENGROESSE': 10, 'VERKAEUFER': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 23. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 93.69168853759766"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.86 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "0552a60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred\n",
      "torch.Size([120, 117, 12])\n",
      "torch.Size([120, 117])\n"
     ]
    }
   ],
   "source": [
    "out = seq_mod3.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "d206fbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'QMPREIS', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['STRASSE', 'O', 'GESAMTPREIS', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'STRASSE', 'O', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'QMPREIS', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'STRASSE', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'VERKAEUFER', 'TERRASSENGROESSE', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'STRASSE', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['ORT', 'STRASSE', 'STRASSE', 'STRASSE', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['ORT', 'STRASSE', 'O', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'TERRASSENGROESSE', 'GESAMTPREIS', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['STRASSE', 'STRASSE', 'O', 'DATUM_VERBUECHERUNG', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'STRASSE', 'STRASSE', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['TERRASSENGROESSE', 'TERRASSENGROESSE', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'GESAMTPREIS', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['ORT', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'STRASSE', 'STRASSE', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "y_pred = seq_mod3.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "4e5622dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6605704077682553\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                  O      0.785     0.930     0.851       643\n",
      "            KAEUFER      0.000     0.000     0.000        18\n",
      "DATUM_VERBUECHERUNG      0.000     0.000     0.000        25\n",
      "      DATUM_VERTRAG      0.000     0.000     0.000        27\n",
      "         VERKAEUFER      0.000     0.000     0.000        24\n",
      "   TERRASSENGROESSE      0.000     0.000     0.000         5\n",
      "        GESAMTPREIS      0.000     0.000     0.000        11\n",
      "            FLAECHE      0.000     0.000     0.000        15\n",
      "           IMMO_TYP      0.000     0.000     0.000        19\n",
      "            QMPREIS      0.000     0.000     0.000        10\n",
      "                ORT      1.000     0.115     0.207        26\n",
      "            STRASSE      0.074     0.125     0.093        16\n",
      "\n",
      "           accuracy                          0.719       839\n",
      "          macro avg      0.155     0.098     0.096       839\n",
      "       weighted avg      0.634     0.719     0.661       839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "classes = seq_mod3.classes()\n",
    "print(metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=classes))\n",
    "sorted_labels = sorted(\n",
    "    classes,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e345cb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5731, grad_fn=<SumBackward0>)\n",
      "tensor([[[-0.4519, -0.1661]]])\n",
      "tensor([[1]])\n",
      "Parameter containing:\n",
      "tensor([[-0.0941,  0.0600],\n",
      "        [-0.0206,  0.0509]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0515, -0.0441], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0194,  0.0469], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "########## SAMPLE CRF ON TOY DATA\n",
    "# this is where I matched my first model score (i.e. log likelihood of crf)\n",
    "torch.manual_seed(1)\n",
    "seq_length = 1  # maximum sequence length in a batch\n",
    "batch_size = 1  # number of samples in the batch\n",
    "num_tags=2\n",
    "model = CRF(num_tags,batch_first=True)\n",
    "#emissions = torch.randn(batch_size, seq_length, num_tags)\n",
    "#tags = torch.tensor([[0, 2, 3], [1, 4, 1]], dtype=torch.long)  # (batch_size, seq_length)\n",
    "emissions = torch.randn(1, seq_length, num_tags)\n",
    "\n",
    "tags = torch.tensor([[1]], dtype=torch.long)  \n",
    "print(model(emissions, tags))\n",
    "print(emissions)\n",
    "print(tags)\n",
    "print(model.transitions)\n",
    "print(model.start_transitions)\n",
    "print(model.end_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "4a9eb50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "tensor(-8.3568, grad_fn=<SumBackward0>)\n",
      "[[3, 4, 3], [4, 1]]\n",
      "tensor(-2.7487, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "########## SAMPLE CRF ON TOY DATA\n",
    "# add mask vector so as to not to consider padding part of sequences\n",
    "# e.g. want to mask last zero of 2nd obs\n",
    "torch.manual_seed(1)\n",
    "seq_length = 3  # maximum sequence length in a batch\n",
    "batch_size = 2  # number of samples in the batch\n",
    "num_tags=5\n",
    "model = CRF(num_tags,batch_first=True)\n",
    "emissions = torch.randn(batch_size, seq_length, num_tags)\n",
    "tags = torch.tensor([[1, 2, 3], [1, 4, 0]], dtype=torch.long)  # (batch_size, seq_length)\n",
    "mask = torch.tensor([[1, 1, 1], [1, 1, 0]], dtype=torch.uint8) # i.e. mask 3rd token of 2nd example in batch\n",
    "print(emissions.shape)\n",
    "print(tags.shape)\n",
    "print(mask.shape)\n",
    "# 1.\n",
    "print(model(emissions, tags, mask=mask)) # model log likelihood\n",
    "# 2.\n",
    "print(model.decode(emissions,mask=mask)) # most likely tag sequences\n",
    "# 3. inference:\n",
    "tags_test = torch.tensor([[1, 2, 3]], dtype=torch.long)  # (batch_size, seq_length)\n",
    "print(model.forward(emissions[0].unsqueeze(dim=0),tags_test)) # returns log likelihood of test tag sequence (larger no. means more likely)\n",
    "# note: need to use torch array w/ same no. of examples as tags_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "d5c17e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "# note: this isn't exactly correct as training examples are shuffled (esp. max len of the smaller 12 ex batch is != 92)\n",
    "auxMax=0\n",
    "x_max_idx=108\n",
    "for i in range(0,min(len(X_train),x_max_idx)):\n",
    "    if len(X_train[i])>auxMax:\n",
    "        auxMax=len(X_train[i])\n",
    "print(auxMax)\n",
    "auxMax2=0\n",
    "x_min_idx=109\n",
    "for i in range(max(0,x_min_idx),len(X_train)):\n",
    "    if len(X_train[i])>auxMax2:\n",
    "        auxMax2=len(X_train[i])\n",
    "print(auxMax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cfe2a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORT', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O']\n",
      "['KAEUFER', 'KAEUFER', 'O', 'O', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "y_pred = seq_mod.predict(X_test)\n",
    "print(y_test[0])\n",
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2960fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=seq_mod.classes_\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d615f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfold all our data - NOTE: this means we don't care about per sentence results. \n",
    "# i.e. each classification is worth same regardless of sentence in which it occurs\n",
    "y_test_unfold = [y for element in y_test for y in element]\n",
    "y_pred_unfold = [y for element in y_pred for y in element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "99efc2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORT', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(y_test_unfold[:10])\n",
    "print(y_pred_unfold[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5de61d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y_test and y_pred into binary formats\n",
    "#from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7f4747c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                  O      0.808     0.888     0.846       643\n",
      "            KAEUFER      0.070     0.278     0.112        18\n",
      "DATUM_VERBUECHERUNG      0.000     0.000     0.000        25\n",
      "      DATUM_VERTRAG      1.000     0.037     0.071        27\n",
      "         VERKAEUFER      0.333     0.042     0.074        24\n",
      "   TERRASSENGROESSE      0.000     0.000     0.000         5\n",
      "        GESAMTPREIS      0.000     0.000     0.000        11\n",
      "            FLAECHE      0.000     0.000     0.000        15\n",
      "           IMMO_TYP      0.000     0.000     0.000        19\n",
      "            QMPREIS      0.000     0.000     0.000        10\n",
      "                ORT      0.000     0.000     0.000        26\n",
      "            STRASSE      0.118     0.125     0.121        16\n",
      "\n",
      "           accuracy                          0.691       839\n",
      "          macro avg      0.194     0.114     0.102       839\n",
      "       weighted avg      0.664     0.691     0.657       839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    y_test_unfold, y_pred_unfold, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d9457a",
   "metadata": {},
   "source": [
    "Now try with leading \"B-\" and \"I-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6f7276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## ONLY RUN IF WE WANT TO ADD LEADING \"B-\" / \"I-\" TO CLASS LABEL\n",
    "# now use above code and loop through all items of annot list:\n",
    "# addLeading=1 for \"Yes\" (i.e. add leading \"B-\",\"I-\" to annot); 0 for \"No\" (i.e. add labels to annot simply as they are)\n",
    "addLeading = 1\n",
    "\n",
    "if addLeading == 1:\n",
    "    for j in range(0,len(annot)):\n",
    "        a = annot[j]\n",
    "        # select list of dict of tokens w/ annnotations and add column w/ no. of words to each dict:\n",
    "        b = a['spans']\n",
    "        # add noWords to b dict. note: b is list of dicts w/ annotations; tokens not on this list don't have annotations\n",
    "        if b!=[]: #i.e. only try to add annotations to tokens if there are annotations to begin with\n",
    "            #print(b)\n",
    "            for i in range(0,len(annot[j]['tokens'])):\n",
    "                    # now break-up label into 1st occurrence (leading \"B-\") and subsequent occurrences (leading \"I-\") (only for non \"O\"'s)\n",
    "                    if annot[j]['tokens'][i]['label'] != \"O\":\n",
    "                        if i==0:\n",
    "                            annot[j]['tokens'][i]['label'] = \"B-\" + annot[j]['tokens'][i]['label']\n",
    "                        else: \n",
    "                            if annot[j]['tokens'][i]['label'] == annot[j]['tokens'][i-1]['label'][2:]: # need to remove the leading \"B-\" that we had already been added to c[i-1]\n",
    "                                annot[j]['tokens'][i]['label'] = \"I-\" + annot[j]['tokens'][i]['label']\n",
    "                            else:\n",
    "                                annot[j]['tokens'][i]['label'] = \"B-\" + annot[j]['tokens'][i]['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e6bd88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get data into format that TorchRNN expects:\n",
    "X=[] \n",
    "y=[]\n",
    "for j in range(0,len(annot)):\n",
    "    a = annot[j]['tokens']\n",
    "    auxX = []\n",
    "    auxy = []\n",
    "    if annot[j]['spans']!=[]: # are there annot for this example?\n",
    "        for i in range(0,len(a)):\n",
    "            #token_element = (a[i]['text'],a[i]['label'])\n",
    "            auxX.append(a[i]['text'])\n",
    "            auxy.append(a[i]['label'])\n",
    "        X.append(auxX)\n",
    "        y.append(auxy)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train, X_test, y_train, y_test = X[:120], X[120:], y[:120], y[120:]\n",
    "vocab = sorted({w for seq in X_train for w in seq}) + [\"$UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "353255ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DORNBIRN', 'In', 'der', 'Schulgasse', 'in', 'Dornbirn', 'hat', 'eine', '71,93', 'Quadratmeter', 'große', 'Wohnung', 'für', 'einen', 'Quadratmeterpreis', 'von', '5533,71', 'Euro', 'den', 'Besitzer', 'gewechselt', '.', 'Dieser', 'beinhaltet', 'auch', 'einen', 'Pkw-Abstellplatz', '.', 'Käufer', 'der', 'Wohnung', 'mit', '9,86', 'Quadratmetern', 'Terrasse', 'ist', 'die', 'ValLiLean', 'Beteiligungs-', 'und', 'Immobilienverwaltungs', 'GmbH', 'Beim', 'Verkäufer', 'handelt', 'es', 'sich', 'um', 'die', 'Karrenblick', 'Projekt', 'GmbH', ' ', 'Der', 'Kaufpreis', 'liegt', 'bei', '398.040', 'Euro', '.', 'Unterzeichnet', 'wurde', 'der', 'Kaufvertrag', 'am', '18.', 'September', '.', 'Die', 'Verbücherung', 'datiert', 'mit', 'Oktober', '2020', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "071c6a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 35. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 2.249159574508667"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.78 s\n",
      "['B-ORT', 'O', 'O', 'B-STRASSE', 'I-STRASSE', 'O', 'B-ORT', 'O', 'O', 'B-FLAECHE', 'O', 'O', 'B-IMMO_TYP', 'O', 'O', 'O', 'O', 'B-QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'B-GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATUM_VERTRAG', 'I-DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'B-DATUM_VERBUECHERUNG', 'I-DATUM_VERBUECHERUNG', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod.fit(X_train, y_train)\n",
    "y_pred = seq_mod.predict(X_test)\n",
    "print(y_test[0])\n",
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "92a8a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=seq_mod.classes_\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b338e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfold all our data - NOTE: this means we don't care about per sentence results. \n",
    "# i.e. each classification is worth same regardless of sentence in which it occurs\n",
    "y_test_unfold = [y for element in y_test for y in element]\n",
    "y_pred_unfold = [y for element in y_pred for y in element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "78515133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                    O      0.773     0.988     0.867       643\n",
      "B-DATUM_VERBUECHERUNG      0.000     0.000     0.000        13\n",
      "I-DATUM_VERBUECHERUNG      0.000     0.000     0.000        12\n",
      "      B-DATUM_VERTRAG      0.000     0.000     0.000        13\n",
      "      I-DATUM_VERTRAG      0.000     0.000     0.000        14\n",
      "            B-FLAECHE      0.000     0.000     0.000        15\n",
      "            I-FLAECHE      0.000     0.000     0.000         0\n",
      "        B-GESAMTPREIS      0.000     0.000     0.000        11\n",
      "        I-GESAMTPREIS      0.000     0.000     0.000         0\n",
      "           B-IMMO_TYP      0.000     0.000     0.000        19\n",
      "           I-IMMO_TYP      0.000     0.000     0.000         0\n",
      "            B-KAEUFER      0.000     0.000     0.000        10\n",
      "            I-KAEUFER      0.000     0.000     0.000         8\n",
      "                B-ORT      0.300     0.115     0.167        26\n",
      "            B-QMPREIS      0.000     0.000     0.000        10\n",
      "            I-QMPREIS      0.000     0.000     0.000         0\n",
      "            B-STRASSE      0.000     0.000     0.000        12\n",
      "            I-STRASSE      0.000     0.000     0.000         4\n",
      "   B-TERRASSENGROESSE      0.000     0.000     0.000         5\n",
      "         B-VERKAEUFER      0.000     0.000     0.000        13\n",
      "         I-VERKAEUFER      0.000     0.000     0.000        11\n",
      "\n",
      "            micro avg      0.760     0.760     0.760       839\n",
      "            macro avg      0.051     0.053     0.049       839\n",
      "         weighted avg      0.601     0.760     0.670       839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasco\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    y_test_unfold, y_pred_unfold, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680fd70",
   "metadata": {},
   "source": [
    "Remove \"B-\" and \"I-\" (in case they are present in labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a2de8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,len(annot)):\n",
    "    a = annot[j]\n",
    "    b = a['spans']\n",
    "    if b!=[]: #i.e. only try to add annotations to tokens if there are annotations to begin with\n",
    "        for i in range(0,len(annot[j]['tokens'])):\n",
    "                if annot[j]['tokens'][i]['label'] != \"O\":\n",
    "                    if annot[j]['tokens'][i]['label'][:2]==\"B-\" or annot[j]['tokens'][i]['label'][:2]==\"I-\":\n",
    "                        annot[j]['tokens'][i]['label']=annot[j]['tokens'][i]['label'][2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f22b42",
   "metadata": {},
   "source": [
    "Try bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "147649d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_mod = TorchRNNSequenceLabeler(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001,\n",
    "    bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "63ca2332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORT', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8413a3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 18. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 2.1157665252685547"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8d5884c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORT', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                  O      0.760     0.893     0.821       643\n",
      "            KAEUFER      0.000     0.000     0.000        18\n",
      "DATUM_VERBUECHERUNG      0.000     0.000     0.000        25\n",
      "      DATUM_VERTRAG      0.000     0.000     0.000        27\n",
      "         VERKAEUFER      0.000     0.000     0.000        24\n",
      "   TERRASSENGROESSE      0.000     0.000     0.000         5\n",
      "        GESAMTPREIS      0.000     0.000     0.000        11\n",
      "            FLAECHE      0.000     0.000     0.000        15\n",
      "           IMMO_TYP      0.000     0.000     0.000        19\n",
      "            QMPREIS      0.000     0.000     0.000        10\n",
      "                ORT      0.000     0.000     0.000        26\n",
      "            STRASSE      0.000     0.000     0.000        16\n",
      "\n",
      "           accuracy                          0.684       839\n",
      "          macro avg      0.063     0.074     0.068       839\n",
      "       weighted avg      0.583     0.684     0.629       839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = seq_mod.predict(X_test)\n",
    "print(y_test[0])\n",
    "print(y_pred[0])\n",
    "\n",
    "labels=seq_mod.classes_\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "\n",
    "# unfold all our data - NOTE: this means we don't care about per sentence results. \n",
    "# i.e. each classification is worth same regardless of sentence in which it occurs\n",
    "y_test_unfold = [y for element in y_test for y in element]\n",
    "y_pred_unfold = [y for element in y_pred for y in element]\n",
    "\n",
    "print(classification_report(\n",
    "    y_test_unfold, y_pred_unfold, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d592b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
