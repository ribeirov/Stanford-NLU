{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d810013b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ce5ffd1ab0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce3e007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "   # max_score = vec[0, argmax(vec)]\n",
    "    max_score = vec[0, torch.argmax(vec,axis=1)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def prepare_sequences(seqs, to_ix):\n",
    "    idxs = [prepare_sequence(seq, to_ix) for seq in seqs]\n",
    "    #return torch.tensor(idxs, dtype=torch.long)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c4bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                # best_tag_id = argmax(next_tag_var)\n",
    "                best_tag_id = torch.argmax(next_tag_var,axis=1)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        # best_tag_id = argmax(terminal_var)\n",
    "        best_tag_id = torch.argmax(terminal_var,axis=1)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "01d37564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([124.4205]), [tensor([4]), tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([10]), tensor([1]), tensor([4]), tensor([9]), tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([10]), tensor([1]), tensor([4]), tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([10]), tensor([1]), tensor([4]), tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([10]), tensor([1]), tensor([4])])\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "(tensor([544.3170]), [tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([0]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([2]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([7]), tensor([7]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([9]), tensor([9]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([11]), tensor([10]), tensor([10]), tensor([11]), tensor([11]), tensor([11])])\n"
     ]
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# Make up some training data\n",
    "#training_data = [(\n",
    "#    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "#    \"B I I I O O O B I O O\".split()\n",
    "#), (\n",
    "#    \"georgia tech is a university in georgia\".split(),\n",
    "#    \"B I O O O O B\".split()\n",
    "#)]\n",
    "\n",
    "#word_to_ix = {}\n",
    "#for sentence, tags in training_data:\n",
    "#    for word in sentence:\n",
    "#        if word not in word_to_ix:\n",
    "#            word_to_ix[word] = len(word_to_ix) # i.e. each successive word added gets a successive index\n",
    "\n",
    "#tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# Check predictions before training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    print(model(precheck_sent))\n",
    "\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(10):\n",
    "        #300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch)\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e08aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
     ]
    }
   ],
   "source": [
    "print(precheck_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dceb4321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DORNBIRN', 'In', 'der', 'Schulgasse', 'in', 'Dornbirn', 'hat', 'eine', '71,93', 'Quadratmeter', 'große', 'Wohnung', 'für', 'einen', 'Quadratmeterpreis', 'von', '5533,71', 'Euro', 'den', 'Besitzer', 'gewechselt', '.', 'Dieser', 'beinhaltet', 'auch', 'einen', 'Pkw-Abstellplatz', '.', 'Käufer', 'der', 'Wohnung', 'mit', '9,86', 'Quadratmetern', 'Terrasse', 'ist', 'die', 'ValLiLean', 'Beteiligungs-', 'und', 'Immobilienverwaltungs', 'GmbH', 'Beim', 'Verkäufer', 'handelt', 'es', 'sich', 'um', 'die', 'Karrenblick', 'Projekt', 'GmbH', ' ', 'Der', 'Kaufpreis', 'liegt', 'bei', '398.040', 'Euro', '.', 'Unterzeichnet', 'wurde', 'der', 'Kaufvertrag', 'am', '18.', 'September', '.', 'Die', 'Verbücherung', 'datiert', 'mit', 'Oktober', '2020', '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f776dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, 'wall': 1, 'street': 2, 'journal': 3, 'reported': 4, 'today': 5, 'that': 6, 'apple': 7, 'corporation': 8, 'made': 9, 'money': 10, 'georgia': 11, 'tech': 12, 'is': 13, 'a': 14, 'university': 15, 'in': 16}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "979b5cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['the', 'wall', 'street', 'journal', 'reported', 'today', 'that', 'apple', 'corporation', 'made', 'money'], ['B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O']), (['georgia', 'tech', 'is', 'a', 'university', 'in', 'georgia'], ['B', 'I', 'O', 'O', 'O', 'O', 'B'])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d029219",
   "metadata": {},
   "source": [
    "Run with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7c88763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "12c08802",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN\n",
    "with open('annotations2.jsonl') as jsonl_file:\n",
    "    # note: after running data-preprocessing.ipynb this file already has token-level labels\n",
    "    lines = jsonl_file.readlines()\n",
    "annot = [json.loads(line) for line in lines]\n",
    "#print(annot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39210650",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN\n",
    "# now convert annotation tokens into list (sentences) of lists (tokens) format for sklearn_crfsuite.CRF\n",
    "train_sents=[] \n",
    "for j in range(0,len(annot)):\n",
    "    a = annot[j]['tokens']\n",
    "    # Only add sample if there are annotations\n",
    "    if annot[j]['spans']!=[]:\n",
    "        train_sentence = []\n",
    "        for i in range(0,len(a)):\n",
    "            if 'label' in a[i]: # only add element if this sample sentence has been labelled \n",
    "                token_element = (a[i]['text'],a[i]['label'])\n",
    "                train_sentence.append(token_element)\n",
    "        train_sents.append(train_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e57bdcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DORNBIRN', 'ORT'), ('In', 'O'), ('der', 'O'), ('Sandgasse', 'STRASSE'), ('2', 'STRASSE'), ('in', 'O'), ('Dornbirn', 'ORT'), ('hat', 'O'), ('eine', 'O'), ('96', 'FLAECHE'), ('Quadratmeter', 'O'), ('große', 'O'), ('Wohnung', 'IMMO_TYP'), ('für', 'O'), ('einen', 'O'), ('Quadratmeterpreis', 'O'), ('von', 'O'), ('3093,75', 'QMPREIS'), ('Euro', 'O'), ('den', 'O'), ('Besitzer', 'O'), ('gewechselt', 'O'), ('.', 'O'), ('Dieser', 'O'), ('beinhaltet', 'O'), ('auch', 'O'), ('einen', 'O'), ('Pkw-Abstellplatz', 'O'), ('.', 'O'), ('Bei', 'O'), ('Käufer', 'O'), ('und', 'O'), ('Verkäufer', 'O'), ('der', 'O'), ('Wohnung', 'O'), ('handelt', 'O'), ('es', 'O'), ('sich', 'O'), ('um', 'O'), ('Privatpersonen', 'VERKAEUFER'), ('.', 'O'), ('Der', 'O'), ('Kaufpreis', 'O'), ('liegt', 'O'), ('bei', 'O'), ('297.000', 'GESAMTPREIS'), ('Euro', 'O'), ('.', 'O'), ('Unterzeichnet', 'O'), ('wurde', 'O'), ('der', 'O'), ('Kaufvertrag', 'O'), ('am', 'O'), ('3.', 'DATUM_VERTRAG'), ('September', 'DATUM_VERTRAG'), ('.', 'O'), ('Die', 'O'), ('Verbücherung', 'O'), ('datiert', 'O'), ('mit', 'O'), ('Oktober', 'DATUM_VERBUECHERUNG'), ('2020', 'DATUM_VERBUECHERUNG'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(train_sents[120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "40df0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into format expected above\n",
    "all_data = [([a1 for a1,a2 in el],[a2 for a1,a2 in el]) for el in train_sents]\n",
    "#tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "tag_to_ix = {'ORT': 0, 'STRASSE': 1, 'FLAECHE': 2, 'IMMO_TYP': 3, 'QMPREIS': 4, 'TERRASSENGROESSE': 5, 'KAEUFER': 6, 'VERKAEUFER': 7, 'GESAMTPREIS': 8, 'DATUM_VERTRAG': 9, 'DATUM_VERBUECHERUNG': 10, 'O': 11, START_TAG: 12, STOP_TAG: 13}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71cd186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "for i in range(0,len(train_sents)):\n",
    "    if len(train_sents[i])==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "05723d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "168f84b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sentence, tags in all_data: # i.e. before splitting training / test data\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix) # i.e. each successive word added gets a successive index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "171d0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training / test.\n",
    "training_data=all_data[:100]\n",
    "test_data=all_data[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "178824db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nenzing', ':', '50', 'ha']\n",
      "tensor([882, 385, 883, 878])\n",
      "['ORT', 'O', 'O', 'O']\n",
      "(tensor([14.7289], grad_fn=<IndexBackward>), [tensor([0]), tensor([11]), tensor([11]), tensor([11])])\n"
     ]
    }
   ],
   "source": [
    "# generate predictions v2\n",
    "\n",
    "sentences = [a for a,_ in test_data]\n",
    "labels = [b for _,b in test_data]\n",
    "print(sentences[0])\n",
    "print(prepare_sequence(sentences[0], word_to_ix))\n",
    "#test_data_preds = model(prepare_sequence(sentences[0], word_to_ix))\n",
    "#print(test_data_preds)\n",
    "print(labels[0])\n",
    "test_data_preds = [model(prepare_sequence(a, word_to_ix)) for a in sentences]\n",
    "print(test_data_preds[0])\n",
    "test_data_gold = [[tag_to_ix[el] for el in a] for a in labels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6b6f5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now unfold both gold and preds list of tensors\n",
    "test_data_preds_unfold = [int(item) for sublist in test_data_preds for item in sublist[1]]\n",
    "test_data_gold_unfold = [int(item) for sublist in test_data_gold for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1deb7733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 11, 11, 11, 0, 11, 11, 11, 0, 11]\n",
      "[0, 11, 11, 11, 11, 11, 11, 11, 0, 11]\n"
     ]
    }
   ],
   "source": [
    "print(test_data_gold_unfold[:10])\n",
    "print(test_data_preds_unfold[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8a587f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'ORT', 1: 'STRASSE', 2: 'FLAECHE', 3: 'IMMO_TYP', 4: 'QMPREIS', 5: 'TERRASSENGROESSE', 6: 'KAEUFER', 7: 'VERKAEUFER', 8: 'GESAMTPREIS', 9: 'DATUM_VERTRAG', 10: 'DATUM_VERBUECHERUNG', 11: 'O', 12: '<START>', 13: '<STOP>'}\n"
     ]
    }
   ],
   "source": [
    "ix_to_tag = {value: key for key, value in tag_to_ix.items()}\n",
    "print(ix_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "4caf77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_preds_unfold_tag = [str(ix_to_tag[el]) for el in test_data_preds_unfold]\n",
    "test_data_gold_unfold_tag = [str(ix_to_tag[el]) for el in test_data_gold_unfold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8624015a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365\n",
      "1365\n",
      "['ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'ORT', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'ORT', 'O', 'O', 'IMMO_TYP', 'ORT', 'O', 'VERKAEUFER', 'VERKAEUFER', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'O', 'O', 'O', 'O', 'STRASSE', 'STRASSE', 'STRASSE', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'O', 'ORT', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'ORT', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['ORT', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'ORT', 'O', 'GESAMTPREIS', 'O', 'ORT', 'O', 'GESAMTPREIS', 'O', 'ORT', 'O', 'GESAMTPREIS', 'O', 'ORT', 'O', 'GESAMTPREIS', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'ORT', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'ORT', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'ORT', 'O', 'GESAMTPREIS', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'ORT', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'ORT', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'IMMO_TYP', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'ORT', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'KAEUFER', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'ORT', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'IMMO_TYP', 'O', 'FLAECHE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'O', 'ORT', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'ORT', 'O', 'O', 'O', 'O', 'ORT', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'FLAECHE', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'KAEUFER', 'KAEUFER', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'IMMO_TYP', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'ORT', 'O', 'O', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'KAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'GESAMTPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O', 'O', 'ORT', 'O', 'O', 'STRASSE', 'STRASSE', 'O', 'ORT', 'O', 'O', 'FLAECHE', 'O', 'O', 'IMMO_TYP', 'O', 'O', 'O', 'O', 'O', 'TERRASSENGROESSE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'VERKAEUFER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QMPREIS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERTRAG', 'DATUM_VERTRAG', 'O', 'O', 'O', 'O', 'O', 'DATUM_VERBUECHERUNG', 'DATUM_VERBUECHERUNG', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data_preds_unfold_tag))\n",
    "print(len(test_data_gold_unfold_tag))\n",
    "print(test_data_preds_unfold_tag)\n",
    "print(test_data_gold_unfold_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ee45b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_distinct = list(tag_to_ix.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "5600f43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'KAEUFER', 'DATUM_VERBUECHERUNG', 'DATUM_VERTRAG', 'VERKAEUFER', 'TERRASSENGROESSE', 'GESAMTPREIS', 'FLAECHE', 'IMMO_TYP', 'QMPREIS', 'ORT', '<START>', '<STOP>', 'STRASSE']\n"
     ]
    }
   ],
   "source": [
    "print(sorted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "dc4efc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0]), tensor([11]), tensor([11]), tensor([11])]\n"
     ]
    }
   ],
   "source": [
    "print(list(test_data_preds[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ff04011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "3417bc7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3492, 2622]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31180/450480238.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 5\u001b[1;33m print(metrics.flat_classification_report(\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtest_data_gold_unfold_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_preds_unfold_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels_distinct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m ))\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\sklearn_crfsuite\\metrics.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(y_true, y_pred, *args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0my_true_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0my_pred_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\sklearn_crfsuite\\metrics.py\u001b[0m in \u001b[0;36mflat_classification_report\u001b[1;34m(y_true, y_pred, labels, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1927\u001b[0m     \"\"\"\n\u001b[0;32m   1928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1929\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\xcs224u\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3492, 2622]"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted( # labels are coming in as id's not names\n",
    "    labels_distinct,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "\n",
    "######################### WHY IS THIS NOT WORKING ???? ########################\n",
    "print(metrics.flat_classification_report(\n",
    "    test_data_gold_unfold_tag, test_data_preds_unfold_tag, labels=labels_distinct, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b3d450ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'KAEUFER', 'DATUM_VERBUECHERUNG', 'DATUM_VERTRAG', 'VERKAEUFER', 'TERRASSENGROESSE', 'GESAMTPREIS', 'FLAECHE', 'IMMO_TYP', 'QMPREIS', 'ORT', '<START>', '<STOP>', 'STRASSE']\n"
     ]
    }
   ],
   "source": [
    "print(sorted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
