{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99fa1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn_crfsuite import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchcrf import CRF\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNDataset, TorchRNNClassifier, TorchRNNModel\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37f28abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotations2.jsonl') as jsonl_file:\n",
    "    # note: after running data-preprocessing.ipynb this file already has token-level labels\n",
    "    lines = jsonl_file.readlines()\n",
    "annot = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "384cb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get data into format that TorchRNN expects:\n",
    "X=[] \n",
    "y=[]\n",
    "for j in range(0,len(annot)):\n",
    "    a = annot[j]['tokens']\n",
    "    auxX = []\n",
    "    auxy = []\n",
    "    if annot[j]['spans']!=[]: # are there annot for this example?\n",
    "        for i in range(0,len(a)):\n",
    "            #token_element = (a[i]['text'],a[i]['label'])\n",
    "            auxX.append(a[i]['text'])\n",
    "            auxy.append(a[i]['label'])\n",
    "        X.append(auxX)\n",
    "        y.append(auxy)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_train, X_test, y_train, y_test = X[:120], X[120:], y[:120], y[120:]\n",
    "vocab = sorted({w for seq in X_train for w in seq}) + [\"$UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "008b403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules\n",
    "import torch_rnn_classifier, torch_model_base\n",
    "import importlib\n",
    "importlib.reload(torch_model_base)\n",
    "importlib.reload(torch_rnn_classifier)\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNModel, TorchRNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22147492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCRFSequenceLabeler_3(TorchRNNClassifier):\n",
    "\n",
    "    def __init__(self,             \n",
    "            vocab,\n",
    "            hidden_dim=50,\n",
    "            embedding=None,\n",
    "            use_embedding=True,\n",
    "            embed_dim=50,\n",
    "            rnn_cell_class=nn.LSTM,\n",
    "            bidirectional=True,\n",
    "            freeze_embedding=False,\n",
    "            classifier_activation=nn.ReLU(),\n",
    "            **base_kwargs):   \n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = embedding\n",
    "        self.use_embedding = use_embedding\n",
    "        self.embed_dim = embed_dim\n",
    "        self.rnn_cell_class = rnn_cell_class\n",
    "        self.bidirectional = bidirectional\n",
    "        self.freeze_embedding = freeze_embedding\n",
    "        self.classifier_activation = classifier_activation\n",
    "        super().__init__(vocab,**base_kwargs)\n",
    "        self.params += [\n",
    "            'hidden_dim',\n",
    "            'embed_dim',\n",
    "            'embedding',\n",
    "            'use_embedding',\n",
    "            'rnn_cell_class',\n",
    "            'bidirectional',\n",
    "            'freeze_embedding',\n",
    "            'classifier_activation']\n",
    "        self.loss = lambda x:x\n",
    "        if self.bidirectional:\n",
    "            self.classifier_dim = self.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.hidden_dim\n",
    "       # self.classifier_layer = nn.Linear(\n",
    "       #     self.classifier_dim, self.n_classes_)\n",
    "\n",
    "       \n",
    "    def build_graph(self): # uses this build_graph instead of TorchRNNClassifier.build_graph\n",
    "       # print(\"here0\")\n",
    "        rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "      #  print(\"here02\")\n",
    "        model = TorchSequenceLabeler_forCRF_3( # this defines self.model\n",
    "            rnn=rnn,\n",
    "            output_dim=self.n_classes_)\n",
    "      #  print(\"here002\")\n",
    "        self.embed_dim = rnn.embed_dim\n",
    "        self.rnn = rnn\n",
    "        return model\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        X, seq_lengths = self._prepare_sequences(X) # converts tokens into tokenIds\n",
    "        if y is None:\n",
    "            return TorchRNNDataset(X, seq_lengths)\n",
    "        else:\n",
    "            # These are the changes from a regular classifier. All\n",
    "            # concern the fact that our labels are sequences of labels.\n",
    "            self.classes_ = sorted({x for seq in y for x in seq})\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            #class2index = dict(zip(self.classes_, range(2,2+self.n_classes_)))\n",
    "            #class2index[STOP_TAG]=0    # add start and stop tags (note: stop needs to be 0 as that is default for padding in collate_fn)\n",
    "            #class2index[START_TAG]=1 \n",
    "            # `y` is a list of tensors of different length. Our Dataset\n",
    "            # class will turn it into a padding tensor for processing.\n",
    "            y = [torch.tensor([class2index[label] for label in seq])\n",
    "                 for seq in y] # converts labels to indices\n",
    "            return TorchRNNDataset(X, seq_lengths, y)\n",
    "\n",
    "    def predict(self, X): # for CRF-RNN X are logits from RNN\n",
    "       # probs = self.predict_proba(X)\n",
    "       # return [[self.classes_[i] for i in seq.argmax(axis=1)] for seq in probs] # seq.argmax(axis=1) gives index of col that maximizes softmax prob\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        seq_lengths = [len(ex) for ex in X]\n",
    "        preds = self._predict(X)     \n",
    "        mask=self.create_mask(seq_lengths).to(device, non_blocking=True) # creates mask matrix (1s are obs used in CRF; 0s are discarded)  \n",
    "        tag_seq = self.crf.decode(preds,mask=mask) # note: X is (nExs,maxTokLen) and here input must be (nExs,maxTokLen,nDistinctTags); out is optimal seq of tagIds\n",
    "        return [[self.classes_[i] for i in seq] for seq in tag_seq] \n",
    "        # see difference vs TorchRNNClassifier.predict\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        flat_preds = [x for seq in preds for x in seq]\n",
    "        flat_y = [x for seq in y for x in seq]\n",
    "        return utils.safe_macro_f1(flat_y, flat_preds)  \n",
    "    \n",
    "    def nClasses(self):\n",
    "        return len(self.classes_)\n",
    "    \n",
    "    def classes(self):\n",
    "        return self.classes_\n",
    "    \n",
    "    def create_mask(self, seq_length):\n",
    "        maxLen=max(seq_length)\n",
    "        auxLen=len(seq_length)\n",
    "        auxOne = torch.ones(maxLen)\n",
    "        auxZero = torch.zeros(maxLen)\n",
    "        auxOne_l=[1]*maxLen\n",
    "        auxZero_l=[0]*maxLen\n",
    "        auxMatrix=[]\n",
    "        for i in range(auxLen):\n",
    "            auxRow=auxOne_l[:seq_length[i]]+auxZero_l[seq_length[i]:]\n",
    "            auxMatrix.append(auxRow)\n",
    "        return torch.tensor(auxMatrix,dtype=torch.uint8)  \n",
    "\n",
    "    \n",
    "    def fit(self, *args):\n",
    "        \"\"\"\n",
    "        Generic optimization method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        *args: list of objects\n",
    "            We assume that the final element of args give the labels\n",
    "            and all the preceding elements give the system inputs.\n",
    "            For regular supervised learning, this is like (X, y), but\n",
    "            we allow for models that might use multiple data structures\n",
    "            for their inputs.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        model: nn.Module or subclass thereof\n",
    "            Set by `build_graph`. If `warm_start=True`, then this is\n",
    "            initialized only by the first call to `fit`.\n",
    "\n",
    "        optimizer: torch.optimizer.Optimizer\n",
    "            Set by `build_optimizer`. If `warm_start=True`, then this is\n",
    "            initialized only by the first call to `fit`.\n",
    "\n",
    "        errors: list of float\n",
    "            List of errors. If `warm_start=True`, then this is\n",
    "            initialized only by the first call to `fit`. Thus, where\n",
    "            `max_iter=5`, if we call `fit` twice with `warm_start=True`,\n",
    "            then `errors` will end up with 10 floats in it.\n",
    "\n",
    "        validation_scores: list\n",
    "            List of scores. This is filled only if `early_stopping=True`.\n",
    "            If `warm_start=True`, then this is initialized only by the\n",
    "            first call to `fit`. Thus, where `max_iter=5`, if we call\n",
    "            `fit` twice with `warm_start=True`, then `validation_scores`\n",
    "            will end up with 10 floats in it.\n",
    "\n",
    "        no_improvement_count: int\n",
    "            Used to control early stopping and convergence. These values\n",
    "            are controlled by `_update_no_improvement_count_early_stopping`\n",
    "            or `_update_no_improvement_count_errors`.  If `warm_start=True`,\n",
    "            then this is initialized only by the first call to `fit`. Thus,\n",
    "            in that situation, the values could accumulate across calls to\n",
    "            `fit`.\n",
    "\n",
    "        best_error: float\n",
    "           Used to control convergence. Smaller is assumed to be better.\n",
    "           If `warm_start=True`, then this is initialized only by the first\n",
    "           call to `fit`. It will be reset by\n",
    "           `_update_no_improvement_count_errors` depending on how the\n",
    "           optimization is proceeding.\n",
    "\n",
    "        best_score: float\n",
    "           Used to control early stopping. If `warm_start=True`, then this\n",
    "           is initialized only by the first call to `fit`. It will be reset\n",
    "           by `_update_no_improvement_count_early_stopping` depending on how\n",
    "           the optimization is proceeding. Important: we currently assume\n",
    "           that larger scores are better. As a result, we will not get the\n",
    "           correct results for, e.g., a scoring function based in\n",
    "           `mean_squared_error`. See `self.score` for additional details.\n",
    "\n",
    "        best_parameters: dict\n",
    "            This is a PyTorch state dict. It is used if and only if\n",
    "            `early_stopping=True`. In that case, it is updated whenever\n",
    "            `best_score` is improved numerically. If the early stopping\n",
    "            criteria are met, then `self.model` is reset to contain these\n",
    "            parameters before `fit` exits.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "      #  print(\"here00\")\n",
    "        if self.early_stopping:\n",
    "            args, dev = self._build_validation_split(\n",
    "                *args, validation_fraction=self.validation_fraction)\n",
    "            \n",
    "\n",
    "        # Dataset:\n",
    "        dataset = self.build_dataset(*args)\n",
    "        dataloader = self._build_dataloader(dataset, shuffle=True)\n",
    "\n",
    "        # Graph:\n",
    "        if not self.warm_start or not hasattr(self, \"model\"):\n",
    "            self.model = self.build_graph()\n",
    "            # This device move has to happen before the optimizer is built:\n",
    "            # https://pytorch.org/docs/master/optim.html#constructing-it\n",
    "            self.model.to(self.device)\n",
    "            self.optimizer = self.build_optimizer()\n",
    "            self.errors = []\n",
    "            self.validation_scores = []\n",
    "            self.no_improvement_count = 0\n",
    "            self.best_error = np.inf\n",
    "            self.best_score = -np.inf\n",
    "            self.best_parameters = None\n",
    "\n",
    "        # Make sure the model is where we want it:\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        self.crf = CRF(self.n_classes_,batch_first=True).to(self.device, non_blocking=True)\n",
    "\n",
    "        for iteration in range(1, self.max_iter+1):\n",
    "\n",
    "            epoch_error = 0.0\n",
    "\n",
    "            for batch_num, batch in enumerate(dataloader, start=1):\n",
    "               # print(\"batch\"+str(batch_num)) \n",
    "\n",
    "               # print(batch)\n",
    "                batch = [x.to(self.device, non_blocking=True) for x in batch]\n",
    "\n",
    "                X_batch = batch[: -1] # list w/ 2 els: 1st el is tensor (108xmaxLen) w/ tokens for each example in batch; 2nd el is (108x1) with lengths of each example\n",
    "                y_batch = batch[-1] # list with each element of this batch (108 el in list) with tensor (maxLen x 1) labels converted to ints and w/ len = maxLen of all example sequences # print(y_batch[0].shape)\n",
    "               # print(X_batch[1].shape)\n",
    "               # print(y_batch[0])\n",
    "               \n",
    "                batch_preds = self.model(*X_batch) # produces logits outputs of lstm\n",
    "               # print(\"batch_preds\")\n",
    "\n",
    "               # print(\"here-model2\")\n",
    "                mask = (self.create_mask(X_batch[1])).to(self.device, non_blocking=True)\n",
    "                #err = self.loss(batch_preds, y_batch) # batch_preds = (108,12,117); y_batch = (108,117)\n",
    "                err = -self.crf(batch_preds,y_batch,mask=mask,reduction='mean') \n",
    "                # NOTE: self.crf outputs log likelihood so we multiply by (-1) so as to minimize this result\n",
    "\n",
    "                if self.gradient_accumulation_steps > 1 and \\\n",
    "                  self.loss.reduction == \"mean\":\n",
    "                    err /= self.gradient_accumulation_steps\n",
    "\n",
    "                err.backward()\n",
    "\n",
    "                epoch_error += err.item()\n",
    "\n",
    "                if batch_num % self.gradient_accumulation_steps == 0 or \\\n",
    "                  batch_num == len(dataloader):\n",
    "                    if self.max_grad_norm is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), self.max_grad_norm)\n",
    "                    # print(\"before\")\n",
    "                    # print(self.model.rnn.rnn.weight_ih_l0) # check if lstm weights are being updated\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    # print(\"after\")\n",
    "                    # print(self.model.rnn.rnn.weight_ih_l0) # check if lstm weights are being updated\n",
    "\n",
    "            # Stopping criteria:\n",
    "\n",
    "            if self.early_stopping:\n",
    "                self._update_no_improvement_count_early_stopping(*dev) # here we max macro avg f1 score (on dev = validation set)\n",
    "                if self.no_improvement_count > self.n_iter_no_change:\n",
    "                    utils.progress_bar(\n",
    "                        \"Stopping after epoch {}. Validation score did \"\n",
    "                        \"not improve by tol={} for more than {} epochs. \"\n",
    "                        \"Final error is {}\".format(iteration, self.tol,\n",
    "                            self.n_iter_no_change, epoch_error),\n",
    "                        verbose=self.display_progress)\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                self._update_no_improvement_count_errors(epoch_error)\n",
    "                if self.no_improvement_count > self.n_iter_no_change:\n",
    "                    utils.progress_bar(\n",
    "                        \"Stopping after epoch {}. Training loss did \"\n",
    "                        \"not improve more than tol={}. Final error \"\n",
    "                        \"is {}.\".format(iteration, self.tol, epoch_error),\n",
    "                        verbose=self.display_progress)\n",
    "                    break\n",
    "\n",
    "            utils.progress_bar(\n",
    "                \"Finished epoch {} of {}; error is {}\".format(\n",
    "                    iteration, self.max_iter, epoch_error),\n",
    "                verbose=self.display_progress)\n",
    "\n",
    "        if self.early_stopping:\n",
    "            self.model.load_state_dict(self.best_parameters)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "727529fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSequenceLabeler_forCRF_3(nn.Module): # no self.hidden_layer or self.classifier_activation as TorchRNNClassifierModel\n",
    "    def __init__(self, rnn, output_dim):\n",
    "       # print(\"here021\")\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.output_dim = output_dim\n",
    "        if self.rnn.bidirectional:\n",
    "            self.classifier_dim = self.rnn.hidden_dim * 2\n",
    "        else:\n",
    "            self.classifier_dim = self.rnn.hidden_dim\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.classifier_dim, self.output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, X, seq_lengths): # X is (noExsInBatch,MaxLen)=(108,117), seq_lengths is the number of tokens in each example in each batch\n",
    "        # this is the forward method of self.model\n",
    "       # print(\"here2\")\n",
    "        outputs, state = self.rnn(X, seq_lengths) # X is (batchSize, maxLen of exs in batch); outputs is (noTokensInEx,hiddDim), state is ((batch_size,1,hiddDim),(batch_size,1,hiddDim)) = (finalHiddState,finalCellState) \n",
    "        outputs, seq_length = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True) # outputs is (batchSize,MaxLen of examples in batch,hidden_dim); seq_length is noTokenInEx for each ex in batch\n",
    "        logits = self.classifier_layer(outputs) # this is an FCL from hidden_dim to output_dim (NoLabelClasses)\n",
    "       # print(logits.shape)\n",
    "        # logits are (108,117,12) or (1,11,5) = (batchSize,MaxLen of examples in batch,noLabelClasses) noLabelClasses include Start + End\n",
    "        return logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af30259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_mod3 = TorchCRFSequenceLabeler_3(\n",
    "    vocab,\n",
    "    early_stopping=True,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "176d21f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 18. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 106.95479583740234"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.18 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = seq_mod3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c60724b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = seq_mod3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4b962dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6513730510909207\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                  O      0.787     0.921     0.849       643\n",
      "            KAEUFER      0.000     0.000     0.000        18\n",
      "DATUM_VERBUECHERUNG      0.000     0.000     0.000        25\n",
      "      DATUM_VERTRAG      0.000     0.000     0.000        27\n",
      "         VERKAEUFER      0.000     0.000     0.000        24\n",
      "   TERRASSENGROESSE      0.000     0.000     0.000         5\n",
      "        GESAMTPREIS      0.056     0.091     0.069        11\n",
      "            FLAECHE      0.000     0.000     0.000        15\n",
      "           IMMO_TYP      0.000     0.000     0.000        19\n",
      "            QMPREIS      0.000     0.000     0.000        10\n",
      "                ORT      0.000     0.000     0.000        26\n",
      "            STRASSE      0.000     0.000     0.000        16\n",
      "\n",
      "           accuracy                          0.707       839\n",
      "          macro avg      0.070     0.084     0.076       839\n",
      "       weighted avg      0.604     0.707     0.651       839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = seq_mod3.classes()\n",
    "print(metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=classes))\n",
    "sorted_labels = sorted(\n",
    "    classes,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
